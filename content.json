{"meta":{"title":"放手の风筝博客","subtitle":"放手の风筝博客","description":"java,java-web,vue,angular,react,spring,spring-boot,spring-cloud,百度,google","author":"kite","url":"https://mykite.github.io"},"pages":[{"title":"","date":"2017-04-17T05:16:47.764Z","updated":"2017-04-17T05:16:47.764Z","comments":true,"path":"index.html","permalink":"https://mykite.github.io/index.html","excerpt":"","text":"title: categories date: 2017-04-17 11:32:49"},{"title":"","date":"2017-04-17T06:13:28.933Z","updated":"2017-04-17T06:13:28.933Z","comments":false,"path":"categories/index.html","permalink":"https://mykite.github.io/categories/index.html","excerpt":"","text":"title: categories date: 2017-04-17 11:32:49"},{"title":"","date":"2017-04-17T05:16:56.587Z","updated":"2017-04-17T05:16:56.587Z","comments":true,"path":"tags/index.html","permalink":"https://mykite.github.io/tags/index.html","excerpt":"","text":"title: tags date: 2017-04-17 11:31:55"},{"title":"about","date":"2017-04-17T05:47:59.000Z","updated":"2017-04-24T08:33:11.000Z","comments":true,"path":"about/index.html","permalink":"https://mykite.github.io/about/index.html","excerpt":"","text":"联系方式： Email:kite_java@163.com,kite.java@gmail.com Wechat:"},{"title":"","date":"2017-06-01T09:41:10.381Z","updated":"2017-06-01T09:41:10.000Z","comments":true,"path":"about/xxx.html","permalink":"https://mykite.github.io/about/xxx.html","excerpt":"","text":"document.forms['pay_form'].submit();"}],"posts":[{"title":"Hystrix超时时间配置","slug":"Hystrix超时时间配置","date":"2017-05-18T07:23:14.045Z","updated":"2017-05-18T08:47:18.000Z","comments":true,"path":"2017/05/18/Hystrix超时时间配置/","link":"","permalink":"https://mykite.github.io/2017/05/18/Hystrix超时时间配置/","excerpt":"","text":"Hystrix配置整体的超时时间hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds=1000 配置参考github 配置单个超时时间hystrix.command.HystrixCommandKey.execution.isolation.thread.timeoutInMilliseconds=1000123456789101112其中HystrixCommandKey为@HystrixCommand 进行配置example@HystrixCommand(groupKey = &quot;StoreSubmission&quot;, commandKey = &quot;StoreSubmission&quot;, threadPoolKey = &quot;StoreSubmission&quot;)public String storeSubmission(ReturnType returnType, InputStream is, String id) &#123;&#125;@HystrixCommand需要引入依赖&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-javanica&lt;/artifactId&gt; &lt;version&gt;$&#123;hystrix-version&#125;&lt;/version&gt;&lt;/dependency&gt; 配置参考stackoverflow 在fegin中配置超时时间在fegin中无法使用@HystrixCommand来单独进行配置1hystrix.command.MyService#getLastTimeData(Map).execution.isolation.thread.timeoutInMilliseconds这种可以的，通过实践验证了 @甲申验证 配置原理HystrixInvocationHandler123456789101112131415161718final class HystrixInvocationHandler implements InvocationHandler &#123; private final Target&lt;?&gt; target; private final Map&lt;Method, MethodHandler&gt; dispatch; private final FallbackFactory&lt;?&gt; fallbackFactory; // Nullable private final Map&lt;Method, Method&gt; fallbackMethodMap; private final Map&lt;Method, Setter&gt; setterMethodMap;//时间配置 static Map&lt;Method, Setter&gt; toSetters(SetterFactory setterFactory, Target&lt;?&gt; target, Set&lt;Method&gt; methods) &#123; Map&lt;Method, Setter&gt; result = new LinkedHashMap&lt;Method, Setter&gt;(); for (Method method : methods) &#123; method.setAccessible(true); result.put(method, setterFactory.create(target, method)); &#125; return result; &#125; SetterFactory1234567891011121314151617181920212223public interface SetterFactory &#123; /** * Returns a hystrix setter appropriate for the given target and method */ HystrixCommand.Setter create(Target&lt;?&gt; target, Method method); /** * Default behavior is to derive the group key from &#123;@link Target#name()&#125; and the command key from * &#123;@link Feign#configKey(Class, Method)&#125;. */ final class Default implements SetterFactory &#123; @Override public HystrixCommand.Setter create(Target&lt;?&gt; target, Method method) &#123; String groupKey = target.name(); String commandKey = Feign.configKey(target.type(), method); return HystrixCommand.Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey(groupKey)) .andCommandKey(HystrixCommandKey.Factory.asKey(commandKey)); &#125; &#125;&#125; fegin12345678910111213public static String configKey(Class targetType, Method method) &#123; StringBuilder builder = new StringBuilder(); builder.append(targetType.getSimpleName()); builder.append(&apos;#&apos;).append(method.getName()).append(&apos;(&apos;); for (Type param : method.getGenericParameterTypes()) &#123; param = Types.resolve(targetType, targetType, param); builder.append(Types.getRawType(param).getSimpleName()).append(&apos;,&apos;); &#125; if (method.getParameterTypes().length &gt; 0) &#123; builder.deleteCharAt(builder.length() - 1); &#125; return builder.append(&apos;)&apos;).toString(); &#125; 对应上面的规则就可以解释了12hystrix.command.MyService#getLastTimeData(Map).execution.isolation.thread.timeoutInMilliseconds其中MyService#getLastTimeData(Map)为commandKey有configKey进行生成 hystrix生效123456HystrixInvocationHandler.invoke-&gt;newHystrixCommand-&gt;execute-&gt;toObservable-&gt;addTimerListener(executionTimeoutInMilliseconds)-&gt;ScheduledThreadPoolExecutor.scheduleAtFixedRate-call-TimerListener.tick-&gt;timeoutRunnable.run-&gt;throw HystrixTimeoutException 简单来说就是创建一个ScheduledThreadPoolExecutor，超时时间后执行抛出异常操作，如果超时时间执行完成会吧这个Reference应用干掉 ps群里看到的问题，闲顺便翻了下源码。有错误请指出，避免误人子弟 参考 配置参考stackoverflow 配置参考github fegin-github Hystrix-github","categories":[{"name":"Hystrix","slug":"Hystrix","permalink":"https://mykite.github.io/categories/Hystrix/"}],"tags":[{"name":"Hystrix","slug":"Hystrix","permalink":"https://mykite.github.io/tags/Hystrix/"},{"name":"springcloud","slug":"springcloud","permalink":"https://mykite.github.io/tags/springcloud/"}]},{"title":"mybatis源码解析-启动配置&使用spring启动配置(一)","slug":"mybatis源码解析-启动配置&使用spring启动配置(一)","date":"2017-05-10T03:12:45.361Z","updated":"2017-05-10T03:14:09.000Z","comments":true,"path":"2017/05/10/mybatis源码解析-启动配置&使用spring启动配置(一)/","link":"","permalink":"https://mykite.github.io/2017/05/10/mybatis源码解析-启动配置&使用spring启动配置(一)/","excerpt":"","text":"什么是mybatis MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以对配置和原生Map使用简单的 XML 或注解，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录 mybatis用来做什么 针对数据库提供便携的高度灵活定制化sql，且使用简单 版本基于mybatis-3.4.4版本 mybatis配置mybatis-config.xml,copy自test下org.apache.ibatis.autoconstructor12345678910111213141516171819202122232425262728&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;!-- autoMappingBehavior should be set in each test case --&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;&gt; &lt;property name=&quot;&quot; value=&quot;&quot;/&gt; &lt;/transactionManager&gt; &lt;dataSource type=&quot;UNPOOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;org.hsqldb.jdbcDriver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:hsqldb:mem:automapping&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;sa&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;org/apache/ibatis/autoconstructor/AutoConstructorMapper.xml&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 构建SqlSessionFactory123456789101112131415 private static SqlSessionFactory sqlSessionFactory;@BeforeClasspublic static void setUp() throws Exception &#123; // create a SqlSessionFactory final Reader reader = Resources.getResourceAsReader(&quot;org/apache/ibatis/autoconstructor/mybatis-config.xml&quot;); sqlSessionFactory = new SqlSessionFactoryBuilder().build(reader);//构建 reader.close(); // populate in-memory database final SqlSession session = sqlSessionFactory.openSession();//获取session final Connection conn = session.getConnection();//打开连接 dbReader.close(); session.close();&#125; 源码分析SqlSessionFactoryBuilder使用构建器模式构建对象1SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(reader); SqlSessionFactoryBuilder12345678910111213141516171819202122232425262728public class SqlSessionFactoryBuilder &#123; public SqlSessionFactory build(Reader reader) &#123; return build(reader, null, null); &#125; public SqlSessionFactory build(Reader reader, String environment, Properties properties) &#123; try &#123; XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); return build(parser.parse()); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException(&quot;Error building SqlSession.&quot;, e); &#125; finally &#123; ErrorContext.instance().reset(); try &#123; reader.close(); &#125; catch (IOException e) &#123; // Intentionally ignore. Prefer previous error. &#125; &#125; &#125; ... public SqlSessionFactory build(Configuration config) &#123; return new DefaultSqlSessionFactory(config); &#125;&#125; XMLConfigBuilder 基于xpath进行解析 propertiesElement typeAliasesElement pluginElement objectFactoryElement objectWrapperFactoryElement reflectorFactoryElement settingsElement environmentsElement databaseIdProviderElement typeHandlerElement mapperElement 具体相关配置请参考官方中文文档1在解析mapper时，如果为resource节点会创建XMLMapperBuilder继续进行parse 12345678910111213141516171819202122232425262728293031323334353637383940414243public class XMLConfigBuilder extends BaseBuilder &#123; private boolean parsed; private XPathParser parser; private String environment; private ReflectorFactory localReflectorFactory = new DefaultReflectorFactory(); public Configuration parse() &#123; if (parsed) &#123; throw new BuilderException(&quot;Each XMLConfigBuilder can only be used once.&quot;); &#125; parsed = true; parseConfiguration(parser.evalNode(&quot;/configuration&quot;));//基于xpath进行解析 return configuration; &#125; private void parseConfiguration(XNode root) &#123; try &#123; //issue #117 read properties first propertiesElement(root.evalNode(&quot;properties&quot;)); Properties settings = settingsAsProperties(root.evalNode(&quot;settings&quot;)); loadCustomVfs(settings); typeAliasesElement(root.evalNode(&quot;typeAliases&quot;)); pluginElement(root.evalNode(&quot;plugins&quot;)); objectFactoryElement(root.evalNode(&quot;objectFactory&quot;)); objectWrapperFactoryElement(root.evalNode(&quot;objectWrapperFactory&quot;)); reflectorFactoryElement(root.evalNode(&quot;reflectorFactory&quot;)); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 environmentsElement(root.evalNode(&quot;environments&quot;)); databaseIdProviderElement(root.evalNode(&quot;databaseIdProvider&quot;)); typeHandlerElement(root.evalNode(&quot;typeHandlers&quot;)); mapperElement(root.evalNode(&quot;mappers&quot;)); &#125; catch (Exception e) &#123; throw new BuilderException(&quot;Error parsing SQL Mapper Configuration. Cause: &quot; + e, e); &#125; &#125; ...&#125; 与spring整合SqlSessionFactoryBean123456&lt;!-- spring和MyBatis，不需要mybatis的配置映射文件 --&gt; &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt; &lt;!-- 自动扫描mapping.xml文件 --&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:dal/**/*.xml&quot;&gt;&lt;/property&gt; &lt;/bean&gt; SqlSessionFactoryBean 实现了FactoryBean通过getObject-&gt;afterPropertiesSet-&gt;buildSqlSessionFactory 进行构建123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221public class SqlSessionFactoryBean implements FactoryBean&lt;SqlSessionFactory&gt;, InitializingBean, ApplicationListener&lt;ApplicationEvent&gt; &#123; private static final Log logger = LogFactory.getLog(SqlSessionFactoryBean.class); private Resource configLocation; private Resource[] mapperLocations; private DataSource dataSource; private TransactionFactory transactionFactory; private Properties configurationProperties; private SqlSessionFactoryBuilder sqlSessionFactoryBuilder = new SqlSessionFactoryBuilder(); private SqlSessionFactory sqlSessionFactory; private String environment = SqlSessionFactoryBean.class.getSimpleName(); // EnvironmentAware requires spring 3.1 private boolean failFast; private Interceptor[] plugins; private TypeHandler&lt;?&gt;[] typeHandlers; private String typeHandlersPackage; private Class&lt;?&gt;[] typeAliases; private String typeAliasesPackage; private Class&lt;?&gt; typeAliasesSuperType; private DatabaseIdProvider databaseIdProvider; // issue #19. No default provider. private ObjectFactory objectFactory; private ObjectWrapperFactory objectWrapperFactory; /** * &#123;@inheritDoc&#125; */ public void afterPropertiesSet() throws Exception &#123; notNull(dataSource, &quot;Property &apos;dataSource&apos; is required&quot;); notNull(sqlSessionFactoryBuilder, &quot;Property &apos;sqlSessionFactoryBuilder&apos; is required&quot;); this.sqlSessionFactory = buildSqlSessionFactory(); &#125; /** * Build a &#123;@code SqlSessionFactory&#125; instance. * * The default implementation uses the standard MyBatis &#123;@code XMLConfigBuilder&#125; API to build a * &#123;@code SqlSessionFactory&#125; instance based on an Reader. * * @return SqlSessionFactory * @throws IOException if loading the config file failed */ protected SqlSessionFactory buildSqlSessionFactory() throws IOException &#123; Configuration configuration; XMLConfigBuilder xmlConfigBuilder = null; if (this.configLocation != null) &#123; xmlConfigBuilder = new XMLConfigBuilder(this.configLocation.getInputStream(), null, this.configurationProperties); configuration = xmlConfigBuilder.getConfiguration(); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Property &apos;configLocation&apos; not specified, using default MyBatis Configuration&quot;); &#125; configuration = new Configuration(); configuration.setVariables(this.configurationProperties); &#125; if (this.objectFactory != null) &#123; configuration.setObjectFactory(this.objectFactory); &#125; if (this.objectWrapperFactory != null) &#123; configuration.setObjectWrapperFactory(this.objectWrapperFactory); &#125; if (hasLength(this.typeAliasesPackage)) &#123; String[] typeAliasPackageArray = tokenizeToStringArray(this.typeAliasesPackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); for (String packageToScan : typeAliasPackageArray) &#123; configuration.getTypeAliasRegistry().registerAliases(packageToScan, typeAliasesSuperType == null ? Object.class : typeAliasesSuperType); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Scanned package: &apos;&quot; + packageToScan + &quot;&apos; for aliases&quot;); &#125; &#125; &#125; if (!isEmpty(this.typeAliases)) &#123; for (Class&lt;?&gt; typeAlias : this.typeAliases) &#123; configuration.getTypeAliasRegistry().registerAlias(typeAlias); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Registered type alias: &apos;&quot; + typeAlias + &quot;&apos;&quot;); &#125; &#125; &#125; if (!isEmpty(this.plugins)) &#123; for (Interceptor plugin : this.plugins) &#123; configuration.addInterceptor(plugin); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Registered plugin: &apos;&quot; + plugin + &quot;&apos;&quot;); &#125; &#125; &#125; if (hasLength(this.typeHandlersPackage)) &#123; String[] typeHandlersPackageArray = tokenizeToStringArray(this.typeHandlersPackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); for (String packageToScan : typeHandlersPackageArray) &#123; configuration.getTypeHandlerRegistry().register(packageToScan); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Scanned package: &apos;&quot; + packageToScan + &quot;&apos; for type handlers&quot;); &#125; &#125; &#125; if (!isEmpty(this.typeHandlers)) &#123; for (TypeHandler&lt;?&gt; typeHandler : this.typeHandlers) &#123; configuration.getTypeHandlerRegistry().register(typeHandler); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Registered type handler: &apos;&quot; + typeHandler + &quot;&apos;&quot;); &#125; &#125; &#125; if (xmlConfigBuilder != null) &#123; try &#123; xmlConfigBuilder.parse(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Parsed configuration file: &apos;&quot; + this.configLocation + &quot;&apos;&quot;); &#125; &#125; catch (Exception ex) &#123; throw new NestedIOException(&quot;Failed to parse config resource: &quot; + this.configLocation, ex); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; if (this.transactionFactory == null) &#123; this.transactionFactory = new SpringManagedTransactionFactory(); &#125; Environment environment = new Environment(this.environment, this.transactionFactory, this.dataSource); configuration.setEnvironment(environment); if (this.databaseIdProvider != null) &#123; try &#123; configuration.setDatabaseId(this.databaseIdProvider.getDatabaseId(this.dataSource)); &#125; catch (SQLException e) &#123; throw new NestedIOException(&quot;Failed getting a databaseId&quot;, e); &#125; &#125; if (!isEmpty(this.mapperLocations)) &#123; for (Resource mapperLocation : this.mapperLocations) &#123; if (mapperLocation == null) &#123; continue; &#125; try &#123; XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(mapperLocation.getInputStream(), configuration, mapperLocation.toString(), configuration.getSqlFragments()); xmlMapperBuilder.parse(); &#125; catch (Exception e) &#123; throw new NestedIOException(&quot;Failed to parse mapping resource: &apos;&quot; + mapperLocation + &quot;&apos;&quot;, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Parsed mapper file: &apos;&quot; + mapperLocation + &quot;&apos;&quot;); &#125; &#125; &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Property &apos;mapperLocations&apos; was not specified or no matching resources found&quot;); &#125; &#125; return this.sqlSessionFactoryBuilder.build(configuration); &#125; /** * &#123;@inheritDoc&#125; */ public SqlSessionFactory getObject() throws Exception &#123; if (this.sqlSessionFactory == null) &#123; afterPropertiesSet(); &#125; return this.sqlSessionFactory; &#125; /** * &#123;@inheritDoc&#125; */ public Class&lt;? extends SqlSessionFactory&gt; getObjectType() &#123; return this.sqlSessionFactory == null ? SqlSessionFactory.class : this.sqlSessionFactory.getClass(); &#125; /** * &#123;@inheritDoc&#125; */ public boolean isSingleton() &#123; return true; &#125;&#125; MapperScannerConfigurer12345&lt;!-- DAO接口所在包名，Spring会自动查找其下的类 --&gt; &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.kite.xxx.mapper&quot; /&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt; &lt;/bean&gt; MapperScannerConfigurer:注册到spring容器中,实现BeanDefinitionRegistryPostProcessor接口spring会进行调用注册123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class MapperScannerConfigurer implements BeanDefinitionRegistryPostProcessor, InitializingBean, ApplicationContextAware, BeanNameAware &#123; private String basePackage; private boolean addToConfig = true; private SqlSessionFactory sqlSessionFactory; private SqlSessionTemplate sqlSessionTemplate; private String sqlSessionFactoryBeanName; private String sqlSessionTemplateBeanName; private Class&lt;? extends Annotation&gt; annotationClass; private Class&lt;?&gt; markerInterface; private ApplicationContext applicationContext; private String beanName; private boolean processPropertyPlaceHolders; private BeanNameGenerator nameGenerator; /** * &#123;@inheritDoc&#125; * * @since 1.0.2 */ public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; if (this.processPropertyPlaceHolders) &#123; processPropertyPlaceHolders(); &#125; ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); scanner.setAddToConfig(this.addToConfig); scanner.setAnnotationClass(this.annotationClass); scanner.setMarkerInterface(this.markerInterface); scanner.setSqlSessionFactory(this.sqlSessionFactory); scanner.setSqlSessionTemplate(this.sqlSessionTemplate); scanner.setSqlSessionFactoryBeanName(this.sqlSessionFactoryBeanName); scanner.setSqlSessionTemplateBeanName(this.sqlSessionTemplateBeanName); scanner.setResourceLoader(this.applicationContext); scanner.setBeanNameGenerator(this.nameGenerator); scanner.registerFilters(); scanner.scan(StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS)); &#125;&#125; 参考 mybatis-github 官方文档","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://mykite.github.io/categories/mybatis/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"https://mykite.github.io/tags/源码分析/"},{"name":"spring","slug":"spring","permalink":"https://mykite.github.io/tags/spring/"},{"name":"mybatis","slug":"mybatis","permalink":"https://mykite.github.io/tags/mybatis/"}]},{"title":"关于h5获取经纬度坑","slug":"关于h5获取经纬度坑","date":"2017-05-08T08:00:43.495Z","updated":"2017-05-08T08:18:25.079Z","comments":true,"path":"2017/05/08/关于h5获取经纬度坑/","link":"","permalink":"https://mykite.github.io/2017/05/08/关于h5获取经纬度坑/","excerpt":"","text":"关于在h5获取经纬度如果在http环境下会出现error，POSITION_UNAVAILABLE地理位置不可用，需要使用https","categories":[{"name":"坑","slug":"坑","permalink":"https://mykite.github.io/categories/坑/"}],"tags":[{"name":"h5","slug":"h5","permalink":"https://mykite.github.io/tags/h5/"}]},{"title":"zipkin-brave提供对dubbo监控插件基于springboot(四)","slug":"zipkin-brave提供对dubbo监控插件基于springboot(四)","date":"2017-04-26T08:52:49.045Z","updated":"2017-08-22T09:34:14.000Z","comments":true,"path":"2017/04/26/zipkin-brave提供对dubbo监控插件基于springboot(四)/","link":"","permalink":"https://mykite.github.io/2017/04/26/zipkin-brave提供对dubbo监控插件基于springboot(四)/","excerpt":"","text":"基于dubbo提供zipkin链路跟踪使用springboot来实现 这里我们可以先查看官方针对其他rpc的实现brave-grpc-3.9.0.jar 原理针对dubbo调用前后进行拦截，创建span,关联parentSpanId,traceId 其中我们要实现4个接口 ClientRequestAdapter ClientResponseAdapter ServerRequestAdapter ServerResponseAdapter DubboClientRequestAdapter实现ClientRequestAdapter123456789101112131415161718192021222324252627282930313233343536373839public class DubboClientRequestAdapter implements ClientRequestAdapter &#123; private Map&lt;String, String&gt; headers; private String spanName; public DubboClientRequestAdapter(@Nullable Map&lt;String, String&gt; headers, @Nullable String spanName) &#123; this.headers = headers; this.spanName = spanName; &#125; @Override public String getSpanName() &#123; return this.spanName; &#125; @Override public void addSpanIdToRequest(SpanId spanId) &#123; if (spanId == null) &#123; headers.put(DubboTraceConst.SAMPLED, &quot;0&quot;); &#125; else &#123; headers.put(DubboTraceConst.SAMPLED, &quot;1&quot;); headers.put(DubboTraceConst.TRACE_ID, IdConversion.convertToString(spanId.traceId)); headers.put(DubboTraceConst.SPAN_ID, IdConversion.convertToString(spanId.spanId)); if (spanId.nullableParentId() != null) &#123; headers.put(DubboTraceConst.PARENT_SPAN_ID, IdConversion.convertToString(spanId.parentId)); &#125; &#125; &#125; @Override public Collection&lt;KeyValueAnnotation&gt; requestAnnotations() &#123; return Collections.emptyList(); &#125; @Override public Endpoint serverAddress() &#123; return null; &#125;&#125; DubboClientResponseAdapter实现ClientResponseAdapter1234567891011121314public class DubboClientResponseAdapter implements ClientResponseAdapter &#123; private StatusEnum status; public DubboClientResponseAdapter(@Nullable StatusEnum status) &#123; this.status = status; &#125; @Override public Collection&lt;KeyValueAnnotation&gt; responseAnnotations() &#123; return Collections.singleton(KeyValueAnnotation.create(DubboTraceConst.STATUS_CODE, status.getDesc())); &#125;&#125; DubboServerRequestAdapter实现ServerRequestAdapter1234567891011121314151617181920212223242526272829303132333435363738394041424344public class DubboServerRequestAdapter implements ServerRequestAdapter &#123; private Map&lt;String, String&gt; headers; private String spanName; public DubboServerRequestAdapter(@Nullable Map&lt;String, String&gt; headers, @Nullable String spanName) &#123; this.headers = headers; this.spanName = spanName; &#125; @Override public TraceData getTraceData() &#123; final String sampled = headers.get(DubboTraceConst.SAMPLED); if (sampled != null) &#123; if (sampled.equals(&quot;0&quot;) || sampled.toLowerCase().equals(&quot;false&quot;)) &#123; return TraceData.builder().sample(false).build(); &#125; else &#123; final String parentSpanId = headers.get(DubboTraceConst.PARENT_SPAN_ID); final String traceId = headers.get(DubboTraceConst.TRACE_ID); final String spanId = headers.get(DubboTraceConst.SPAN_ID); if (traceId != null &amp;&amp; spanId != null) &#123; SpanId span = getSpanId(traceId, spanId, parentSpanId); return TraceData.builder().sample(true).spanId(span).build(); &#125; &#125; &#125; return TraceData.builder().build(); &#125; @Override public String getSpanName() &#123; return this.spanName; &#125; @Override public Collection&lt;KeyValueAnnotation&gt; requestAnnotations() &#123; return Collections.emptyList(); &#125; static SpanId getSpanId(String traceId, String spanId, String parentSpanId) &#123; return SpanId.builder().traceId(convertToLong(traceId)).spanId(convertToLong(spanId)) .parentId(parentSpanId == null ? null : convertToLong(parentSpanId)).build(); &#125;&#125; DubboServerResponseAdapter实现ServerResponseAdapter1234567891011121314public class DubboServerResponseAdapter implements ServerResponseAdapter &#123; private StatusEnum status; public DubboServerResponseAdapter(@Nullable StatusEnum status) &#123; this.status = status; &#125; @Override public Collection&lt;KeyValueAnnotation&gt; responseAnnotations() &#123; return Collections.singleton(KeyValueAnnotation.create(DubboTraceConst.STATUS_CODE, status.getDesc())); &#125; &#125; dubbo调用拦截 dubbo的调用会执行filterChain,其中区分PROVIDER，CONSUMER 所以可以记录对应的四个时间 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104@Activate(group = &#123;Constants.PROVIDER, Constants.CONSUMER&#125;)public class BraveDubboFilter implements Filter &#123; /** * @tips:这里不要用注解的方式 */ private ClientRequestInterceptor clientRequestInterceptor; private ClientResponseInterceptor clientResponseInterceptor; private ServerRequestInterceptor serverRequestInterceptor; private ServerResponseInterceptor serverResponseInterceptor; public void setClientRequestInterceptor(ClientRequestInterceptor clientRequestInterceptor) &#123; this.clientRequestInterceptor = clientRequestInterceptor; &#125; public BraveDubboFilter setClientResponseInterceptor(ClientResponseInterceptor clientResponseInterceptor) &#123; this.clientResponseInterceptor = clientResponseInterceptor; return this; &#125; public BraveDubboFilter setServerRequestInterceptor(ServerRequestInterceptor serverRequestInterceptor) &#123; this.serverRequestInterceptor = serverRequestInterceptor; return this; &#125; public BraveDubboFilter setServerResponseInterceptor(ServerResponseInterceptor serverResponseInterceptor) &#123; this.serverResponseInterceptor = serverResponseInterceptor; return this; &#125; public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; /* * 监控的 dubbo 服务，不纳入跟踪范围 */ if (&quot;com.alibaba.dubbo.monitor.MonitorService&quot;.equals(invoker.getInterface().getName())) &#123; return invoker.invoke(invocation); &#125; RpcContext context = RpcContext.getContext(); /* * 调用的方法名 以此作为 span name */ String methodName = invocation.getMethodName(); /* * provider 应用相关信息 */ StatusEnum status = StatusEnum.OK; if (&quot;0&quot;.equals(invocation.getAttachment(DubboTraceConst.SAMPLED)) || &quot;false&quot;.equals(invocation.getAttachment(DubboTraceConst.SAMPLED))) &#123; return invoker.invoke(invocation); &#125; //注入 if(!inject()) &#123; return invoker.invoke(invocation); &#125; if (context.isConsumerSide()) &#123; System.out.println(&quot;consumer execute&quot;); /* * Client side */ clientRequestInterceptor.handle(new DubboClientRequestAdapter(invocation.getAttachments(), methodName)); Result result = null; try &#123; result = invoker.invoke(invocation); &#125; catch (RpcException e) &#123; status = StatusEnum.ERROR; throw e; &#125; finally &#123; final DubboClientResponseAdapter clientResponseAdapter = new DubboClientResponseAdapter(status); clientResponseInterceptor.handle(clientResponseAdapter); &#125; return result; &#125; else if (context.isProviderSide()) &#123; System.out.println(&quot;provider execute&quot;); serverRequestInterceptor.handle(new DubboServerRequestAdapter(context.getAttachments(), methodName)); Result result = null; try &#123; result = invoker.invoke(invocation); &#125; finally &#123; serverResponseInterceptor.handle(new DubboServerResponseAdapter(status)); &#125; return result; &#125; return invoker.invoke(invocation); &#125; private boolean inject() &#123; Brave brave = ApplicationContextHolder.getBean(Brave.class); if(brave == null) &#123; return false; &#125; this.setClientRequestInterceptor(brave.clientRequestInterceptor()); this.setClientResponseInterceptor(brave.clientResponseInterceptor()); this.setServerRequestInterceptor(brave.serverRequestInterceptor()); this.setServerResponseInterceptor(brave.serverResponseInterceptor()); return true; &#125;&#125; 使用springboot configuration基于注解启用1234567@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(DubboTraceConfiguration.class)public @interface EnableDubboTrace &#123;&#125; 配置项12345678910@Configuration@ConditionalOnClass(Brave.class)public class DubboTraceConfiguration &#123; @Bean public ApplicationContextAware holder() &#123; return new ApplicationContextHolder(); &#125; &#125; ApplicationContextHolder1234567891011121314151617181920public class ApplicationContextHolder implements ApplicationContextAware &#123; private static ApplicationContext applicationContext; public void setApplicationContext(ApplicationContext ctx) throws BeansException &#123; setCtx(ctx); &#125; private static void setCtx(ApplicationContext ctx) &#123; applicationContext = ctx; &#125; public static &lt;T&gt; T getBean(Class&lt;T&gt; requiredType)&#123; return applicationContext.getBean(requiredType); &#125; public static Object getBean(String classStr) &#123; return applicationContext.getBean(classStr); &#125;&#125; 其他类1234567891011121314151617181920212223242526272829303132333435public interface DubboTraceConst &#123; String SAMPLED = &quot;dubbo.trace.sampled&quot;; String PARENT_SPAN_ID = &quot;dubbo.trace.parentSpanId&quot;; String SPAN_ID = &quot;dubbo.trace.spanId&quot;; String TRACE_ID = &quot;dubbo.trace.traceId&quot;; String STATUS_CODE = &quot;dubbo.trace.staus_code&quot;;&#125;public enum StatusEnum &#123; OK(200, &quot;OK&quot;), ERROR(500, &quot;ERROR&quot;); private int code; private String desc; private StatusEnum(int code, String desc) &#123; this.code = code; this.desc = desc; &#125; public int getCode() &#123; return code; &#125; public String getDesc() &#123; return desc; &#125;&#125; 针对dubbo filter进行配置文件添加12src/main/resources/META-INF/dubbo/com.alibaba.dubbo.rpc.FilterBraveDubboFilter=com.kite.zipkin.filter.BraveDubboFilter 如何使用 ps:前置条件是已经有了Brave 导入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.kite.zipkin&lt;/groupId&gt; &lt;artifactId&gt;dubbo-zipkin-spring-starter&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt; 前置条件配置12345678910111213141516171819202122232425262728@Configurationpublic class ZipkinConfig &#123; //span（一次请求信息或者一次链路调用）信息收集器 @Bean public SpanCollector spanCollector() &#123; Config config = HttpSpanCollector.Config.builder() .compressionEnabled(false)// 默认false，span在transport之前是否会被gzipped .connectTimeout(5000) .flushInterval(1) .readTimeout(6000) .build(); return HttpSpanCollector.create(&quot;http://localhost:9411&quot;, config, new EmptySpanCollectorMetricsHandler()); &#125; //作为各调用链路，只需要负责将指定格式的数据发送给zipkin @Bean public Brave brave(SpanCollector spanCollector)&#123; Builder builder = new Builder(&quot;service1&quot;);//指定serviceName builder.spanCollector(spanCollector); builder.traceSampler(Sampler.create(1));//采集率 return builder.build(); &#125; &#125; 启动dubboTrace1234567@SpringBootApplication@EnableDubboTracepublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 实现效果 ps 如果需要添加其他信息在requestAnnotations()里面进行添加 当前实现方式为依赖应用方提供Brave配置，如果不想由应用方提供可以使用springboot的，@ConditionalOnMissingBean来进行创建链接 zipkin简单介绍及环境搭建（一） zipkin-server源码分析（二） zipkin-java-brave源码分析（三）","categories":[{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/categories/zipkin/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"https://mykite.github.io/tags/源码分析/"},{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/tags/springboot/"},{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/tags/zipkin/"},{"name":"教程","slug":"教程","permalink":"https://mykite.github.io/tags/教程/"}]},{"title":"zipkin-java-brave源码分析（三）","slug":"zipkin-java-brave源码分析（三）","date":"2017-04-25T07:44:22.590Z","updated":"2017-08-22T09:34:05.408Z","comments":true,"path":"2017/04/25/zipkin-java-brave源码分析（三）/","link":"","permalink":"https://mykite.github.io/2017/04/25/zipkin-java-brave源码分析（三）/","excerpt":"","text":"什么是brave brave是zipkin官方提供的java版本zipkin-client实现 brave提供的功能基于brave3.9.01234567891011121314151617181920212223&lt;modules&gt; &lt;module&gt;brave-core&lt;/module&gt; &lt;module&gt;brave-benchmarks&lt;/module&gt; &lt;module&gt;brave-http&lt;/module&gt; &lt;module&gt;brave-core-spring&lt;/module&gt; &lt;module&gt;brave-resteasy-spring&lt;/module&gt; &lt;module&gt;brave-resteasy3-spring&lt;/module&gt; &lt;module&gt;brave-spancollector-http&lt;/module&gt; &lt;module&gt;brave-spancollector-scribe&lt;/module&gt; &lt;module&gt;brave-spancollector-kafka&lt;/module&gt; &lt;module&gt;brave-spancollector-local&lt;/module&gt; &lt;module&gt;brave-sampler-zookeeper&lt;/module&gt; &lt;module&gt;brave-jersey&lt;/module&gt; &lt;module&gt;brave-jersey2&lt;/module&gt; &lt;module&gt;brave-jaxrs2&lt;/module&gt; &lt;module&gt;brave-grpc&lt;/module&gt; &lt;module&gt;brave-apache-http-interceptors&lt;/module&gt; &lt;module&gt;brave-spring-web-servlet-interceptor&lt;/module&gt; &lt;module&gt;brave-spring-resttemplate-interceptors&lt;/module&gt; &lt;module&gt;brave-mysql&lt;/module&gt; &lt;module&gt;brave-web-servlet-filter&lt;/module&gt; &lt;module&gt;brave-okhttp&lt;/module&gt; &lt;/modules&gt; 基于http提供brave源码分析brave-spancollector-http提供httpCollector收集器 brave-web-servlet-filter基于http请求提供过滤器 brave-apache-http-interceptors基于apache-http-client发起气球提供拦截器 基于springboot启动step1配置 针对collector的SpanCollector 针对http请求的filter BraveServletFilter 针对数据发送的Brave 针对http-client请求的拦截器BraveHttpRequestInterceptor，BraveHttpResponseInterceptor12345678910111213141516171819202122232425262728293031323334353637383940414243@Configurationpublic class ZipkinConfig &#123; //span（一次请求信息或者一次链路调用）信息收集器 @Bean public SpanCollector spanCollector() &#123; Config config = HttpSpanCollector.Config.builder() .compressionEnabled(false)// 默认false，span在transport之前是否会被gzipped .connectTimeout(5000) .flushInterval(1) .readTimeout(6000) .build(); return HttpSpanCollector.create(&quot;http://localhost:9411&quot;, config, new EmptySpanCollectorMetricsHandler()); &#125; //作为各调用链路，只需要负责将指定格式的数据发送给zipkin @Bean public Brave brave(SpanCollector spanCollector)&#123; Builder builder = new Builder(&quot;service1&quot;);//指定serviceName builder.spanCollector(spanCollector); builder.traceSampler(Sampler.create(1));//采集率 return builder.build(); &#125; //设置server的（服务端收到请求和服务端完成处理，并将结果发送给客户端）过滤器 @Bean public BraveServletFilter braveServletFilter(Brave brave) &#123; BraveServletFilter filter = new BraveServletFilter(brave.serverRequestInterceptor(), brave.serverResponseInterceptor(), new DefaultSpanNameProvider()); return filter; &#125; //设置client的（发起请求和获取到服务端返回信息）拦截器 @Bean public CloseableHttpClient okHttpClient(Brave brave)&#123; CloseableHttpClient httpclient = HttpClients.custom() .addInterceptorFirst(new BraveHttpRequestInterceptor(brave.clientRequestInterceptor(), new DefaultSpanNameProvider())) .addInterceptorFirst(new BraveHttpResponseInterceptor(brave.clientResponseInterceptor())) .build(); return httpclient; &#125; &#125; 基于http发起请求服务端处理post or get url : http://localhost/service1 相关代码请查看 zipkin简单介绍及环境搭建（一） 流程图point123456789101112131415161718192021222324252627针对请求，如果Sampledheader包含(X-B3-Sampled)会获取header中的ParentSpanId,TraceId,SpanId直接返回，否者会认为这是一个新的请求会构建SpanHttpServerRequestAdapter.getTraceData()public TraceData getTraceData() &#123; final String sampled = serverRequest.getHttpHeaderValue(BraveHttpHeaders.Sampled.getName()); if (sampled != null) &#123; if (sampled.equals(&quot;0&quot;) || sampled.toLowerCase().equals(&quot;false&quot;)) &#123; return TraceData.builder().sample(false).build(); &#125; else &#123; final String parentSpanId = serverRequest.getHttpHeaderValue(BraveHttpHeaders.ParentSpanId.getName()); final String traceId = serverRequest.getHttpHeaderValue(BraveHttpHeaders.TraceId.getName()); final String spanId = serverRequest.getHttpHeaderValue(BraveHttpHeaders.SpanId.getName()); if (traceId != null &amp;&amp; spanId != null) &#123; SpanId span = getSpanId(traceId, spanId, parentSpanId); return TraceData.builder().sample(true).spanId(span).build(); &#125; &#125; &#125; return TraceData.builder().build(); &#125;针对请求的采样traceSampler().isSampled(newTraceId)，没有使用zk情况下CountingSampler来决定public synchronized boolean isSampled(long traceIdIgnored) &#123; boolean result = sampleDecisions.get(i++); if (i == 100) i = 0; return result; &#125; 基于apache-http发起请求流程图 如何在代码中添加自己的annotation or binaryAnnotation直接注入Brave即可 ps(不建议这样做，代码侵入。 zipkin不建议添加大量数据)123456789101112131415161718192021@RestControllerpublic class ZipkinBraveController &#123; @Autowired private CloseableHttpClient httpClient; @Autowired private com.github.kristofa.brave.Brave brave; @GetMapping(&quot;/service1&quot;) public String myboot() throws Exception &#123; brave.serverTracer().submitBinaryAnnotation(&quot;状态&quot;, &quot;成功&quot;); Thread.sleep(100);//100ms HttpGet get = new HttpGet(&quot;http://localhost:81/test&quot;); CloseableHttpResponse execute = httpClient.execute(get); /* * 1、执行execute()的前后，会执行相应的拦截器（cs,cr） * 2、请求在被调用方执行的前后，也会执行相应的拦截器（sr,ss） */ return EntityUtils.toString(execute.getEntity(), &quot;utf-8&quot;); &#125;&#125; ps 如果collector要使用kafka直接切换spanController即可，需要server端进行对应配置123456789101112131415client端 KafkaSpanCollector.create(KafkaSpanCollector.Config.builder().kafkaProperties(null).build(), new EmptySpanCollectorMetricsHandler());server端需要配置kafka配置final class KafkaZooKeeperSetCondition extends SpringBootCondition &#123; static final String PROPERTY_NAME = &quot;zipkin.collector.kafka.zookeeper&quot;; @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata a) &#123; String kafkaZookeeper = context.getEnvironment().getProperty(PROPERTY_NAME); return kafkaZookeeper == null || kafkaZookeeper.isEmpty() ? ConditionOutcome.noMatch(PROPERTY_NAME + &quot; isn&apos;t set&quot;) : ConditionOutcome.match(); &#125;&#125; 链接 zipkin简单介绍及环境搭建（一） zipkin-server源码分析（二）","categories":[{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/categories/zipkin/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"https://mykite.github.io/tags/源码分析/"},{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/tags/springboot/"},{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/tags/zipkin/"},{"name":"教程","slug":"教程","permalink":"https://mykite.github.io/tags/教程/"}]},{"title":"zipkin-server源码分析（二）","slug":"zipkin-server源码分析（二）","date":"2017-04-24T07:40:20.425Z","updated":"2017-04-24T07:43:48.000Z","comments":true,"path":"2017/04/24/zipkin-server源码分析（二）/","link":"","permalink":"https://mykite.github.io/2017/04/24/zipkin-server源码分析（二）/","excerpt":"","text":"源码地址 github zipkin-server配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149zipkin: self-tracing: # Set to true to enable self-tracing. enabled: $&#123;SELF_TRACING_ENABLED:false&#125; # percentage to self-traces to retain sample-rate: $&#123;SELF_TRACING_SAMPLE_RATE:1.0&#125; # Interval in seconds to flush self-tracing data to storage. flush-interval: $&#123;SELF_TRACING_FLUSH_INTERVAL:1&#125; collector: # percentage to traces to retain sample-rate: $&#123;COLLECTOR_SAMPLE_RATE:1.0&#125; kafka: # ZooKeeper host string, comma-separated host:port value. zookeeper: $&#123;KAFKA_ZOOKEEPER:&#125; # Name of topic to poll for spans topic: $&#123;KAFKA_TOPIC:zipkin&#125; # Consumer group this process is consuming on behalf of. group-id: $&#123;KAFKA_GROUP_ID:zipkin&#125; # Count of consumer threads consuming the topic streams: $&#123;KAFKA_STREAMS:1&#125; # Maximum size of a message containing spans in bytes max-message-size: $&#123;KAFKA_MAX_MESSAGE_SIZE:1048576&#125; scribe: enabled: $&#123;SCRIBE_ENABLED:false&#125; category: zipkin port: $&#123;COLLECTOR_PORT:9410&#125; query: # 7 days in millis lookback: $&#123;QUERY_LOOKBACK:86400000&#125; # The Cache-Control max-age (seconds) for /api/v1/services and /api/v1/spans names-max-age: 300 # CORS allowed-origins. allowed-origins: &quot;*&quot; storage: strict-trace-id: $&#123;STRICT_TRACE_ID:true&#125; type: $&#123;STORAGE_TYPE:mem&#125; cassandra: # Comma separated list of hosts / ip addresses part of Cassandra cluster. contact-points: $&#123;CASSANDRA_CONTACT_POINTS:localhost&#125; # Name of the datacenter that will be considered &quot;local&quot; for latency load balancing. When unset, load-balancing is round-robin. local-dc: $&#123;CASSANDRA_LOCAL_DC:&#125; # Will throw an exception on startup if authentication fails. username: $&#123;CASSANDRA_USERNAME:&#125; password: $&#123;CASSANDRA_PASSWORD:&#125; keyspace: $&#123;CASSANDRA_KEYSPACE:zipkin&#125; # Max pooled connections per datacenter-local host. max-connections: $&#123;CASSANDRA_MAX_CONNECTIONS:8&#125; # Ensuring that schema exists, if enabled tries to execute script /zipkin-cassandra-core/resources/cassandra-schema-cql3.txt. ensure-schema: $&#123;CASSANDRA_ENSURE_SCHEMA:true&#125; # 7 days in seconds span-ttl: $&#123;CASSANDRA_SPAN_TTL:604800&#125; # 3 days in seconds index-ttl: $&#123;CASSANDRA_INDEX_TTL:259200&#125; # the maximum trace index metadata entries to cache index-cache-max: $&#123;CASSANDRA_INDEX_CACHE_MAX:100000&#125; # how long to cache index metadata about a trace. 1 minute in seconds index-cache-ttl: $&#123;CASSANDRA_INDEX_CACHE_TTL:60&#125; # how many more index rows to fetch than the user-supplied query limit index-fetch-multiplier: $&#123;CASSANDRA_INDEX_FETCH_MULTIPLIER:3&#125; # Using ssl for connection, rely on Keystore use-ssl: $&#123;CASSANDRA_USE_SSL:false&#125; cassandra3: # Comma separated list of hosts / ip addresses part of Cassandra cluster. contact-points: $&#123;CASSANDRA3_CONTACT_POINTS:localhost&#125; # Name of the datacenter that will be considered &quot;local&quot; for latency load balancing. When unset, load-balancing is round-robin. local-dc: $&#123;CASSANDRA3_LOCAL_DC:&#125; # Will throw an exception on startup if authentication fails. username: $&#123;CASSANDRA3_USERNAME:&#125; password: $&#123;CASSANDRA3_PASSWORD:&#125; keyspace: $&#123;CASSANDRA3_KEYSPACE:zipkin3&#125; # Max pooled connections per datacenter-local host. max-connections: $&#123;CASSANDRA3_MAX_CONNECTIONS:8&#125; # Ensuring that schema exists, if enabled tries to execute script /cassandra3-schema.cql ensure-schema: $&#123;CASSANDRA3_ENSURE_SCHEMA:true&#125; # how many more index rows to fetch than the user-supplied query limit index-fetch-multiplier: $&#123;CASSANDRA3_INDEX_FETCH_MULTIPLIER:3&#125; # Using ssl for connection, rely on Keystore use-ssl: $&#123;CASSANDRA3_USE_SSL:false&#125; elasticsearch: # host is left unset intentionally, to defer the decision hosts: $&#123;ES_HOSTS:&#125; pipeline: $&#123;ES_PIPELINE:&#125; max-requests: $&#123;ES_MAX_REQUESTS:64&#125; aws: domain: $&#123;ES_AWS_DOMAIN:&#125; region: $&#123;ES_AWS_REGION:&#125; index: $&#123;ES_INDEX:zipkin&#125; date-separator: $&#123;ES_DATE_SEPARATOR:-&#125; index-shards: $&#123;ES_INDEX_SHARDS:5&#125; index-replicas: $&#123;ES_INDEX_REPLICAS:1&#125; username: $&#123;ES_USERNAME:&#125; password: $&#123;ES_PASSWORD:&#125; mysql: host: $&#123;MYSQL_HOST:localhost&#125; port: $&#123;MYSQL_TCP_PORT:3306&#125; username: $&#123;MYSQL_USER:&#125; password: $&#123;MYSQL_PASS:&#125; db: $&#123;MYSQL_DB:zipkin&#125; max-active: $&#123;MYSQL_MAX_CONNECTIONS:10&#125; use-ssl: $&#123;MYSQL_USE_SSL:false&#125; ui: ## Values below here are mapped to ZipkinUiProperties, served as /config.json # Default limit for Find Traces query-limit: 10 # The value here becomes a label in the top-right corner environment: # Default duration to look back when finding traces. # Affects the &quot;Start time&quot; element in the UI. 1 hour in millis default-lookback: 3600000 # Which sites this Zipkin UI covers. Regex syntax. (e.g. http:\\/\\/example.com\\/.*) # Multiple sites can be specified, e.g. # - .*example1.com # - .*example2.com # Default is &quot;match all websites&quot; instrumented: .*server: port: $&#123;QUERY_PORT:9411&#125; compression: enabled: true # compresses any response over min-response-size (default is 2KiB) # Includes dynamic json content and large static assets from zipkin-ui mime-types: application/json,application/javascript,text/css,image/svgspring: mvc: favicon: # zipkin has its own favicon enabled: false autoconfigure: exclude: # otherwise we might initialize even when not needed (ex when storage type is cassandra) - org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfigurationinfo: zipkin: version: &quot;@project.version@&quot;logging: level: # Silence Invalid method name: &apos;__can__finagle__trace__v3__&apos; com.facebook.swift.service.ThriftServiceProcessor: &apos;OFF&apos;# # investigate /api/v1/dependencies# zipkin.internal.DependencyLinker: &apos;DEBUG&apos;# # log cassandra queries (DEBUG is without values)# com.datastax.driver.core.QueryLogger: &apos;TRACE&apos;# # log cassandra trace propagation# com.datastax.driver.core.Message: &apos;TRACE&apos; zipkin-server启动 zipkin基于springboot zipkin-server12345678910111213141516171819@SpringBootApplication@EnableZipkinServerpublic class ZipkinServer &#123; public static void main(String[] args) &#123; new SpringApplicationBuilder(ZipkinServer.class) .listeners(new RegisterZipkinHealthIndicators()) .properties(&quot;spring.config.name=zipkin-server&quot;).run(args); &#125;&#125;导入@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(&#123;ZipkinServerConfiguration.class, BraveConfiguration.class, ZipkinQueryApiV1.class, ZipkinHttpCollector.class&#125;)public @interface EnableZipkinServer &#123;&#125; step1.构建存储 StorageComponent SpanStore123456789101112131415161718192021222324storage: strict-trace-id: $&#123;STRICT_TRACE_ID:true&#125; type: $&#123;STORAGE_TYPE:mem&#125;配置文件默认使用为mem内存存储可以修改 -XX为springboot对应配置-XXstorage.type=对应存储结构@Configurationpublic class ZipkinServerConfiguration &#123; ... //对应默认存储配置，只有当zipkin.storage.type=mem才会执行 @Configuration // &quot;matchIfMissing = true&quot; ensures this is used when there&apos;s no configured storage type @ConditionalOnProperty(name = &quot;zipkin.storage.type&quot;, havingValue = &quot;mem&quot;, matchIfMissing = true) @ConditionalOnMissingBean(StorageComponent.class) static class InMemoryConfiguration &#123; @Bean StorageComponent storage(@Value(&quot;$&#123;zipkin.storage.strict-trace-id:true&#125;&quot;) boolean strictTraceId) &#123; return InMemoryStorage.builder().strictTraceId(strictTraceId).build(); &#125; &#125;&#125; 提供api生成tracerest入口1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@RestController@CrossOrigin(&quot;$&#123;zipkin.query.allowed-origins:*&#125;&quot;)public class ZipkinHttpCollector &#123; static final ResponseEntity&lt;?&gt; SUCCESS = ResponseEntity.accepted().build(); static final String APPLICATION_THRIFT = &quot;application/x-thrift&quot;; final CollectorMetrics metrics; final Collector collector; @Autowired ZipkinHttpCollector(StorageComponent storage, CollectorSampler sampler, CollectorMetrics metrics) &#123; this.metrics = metrics.forTransport(&quot;http&quot;); this.collector = Collector.builder(getClass()) .storage(storage).sampler(sampler).metrics(this.metrics).build(); &#125; //入口 @RequestMapping(value = &quot;/api/v1/spans&quot;, method = POST) public ListenableFuture&lt;ResponseEntity&lt;?&gt;&gt; uploadSpansJson( @RequestHeader(value = &quot;Content-Encoding&quot;, required = false) String encoding, @RequestBody byte[] body ) &#123; return validateAndStoreSpans(encoding, Codec.JSON, body); &#125; @RequestMapping(value = &quot;/api/v1/spans&quot;, method = POST, consumes = APPLICATION_THRIFT) public ListenableFuture&lt;ResponseEntity&lt;?&gt;&gt; uploadSpansThrift( @RequestHeader(value = &quot;Content-Encoding&quot;, required = false) String encoding, @RequestBody byte[] body ) &#123; return validateAndStoreSpans(encoding, Codec.THRIFT, body); &#125; ListenableFuture&lt;ResponseEntity&lt;?&gt;&gt; validateAndStoreSpans(String encoding, Codec codec, byte[] body) &#123; SettableListenableFuture&lt;ResponseEntity&lt;?&gt;&gt; result = new SettableListenableFuture&lt;&gt;(); metrics.incrementMessages(); if (encoding != null &amp;&amp; encoding.contains(&quot;gzip&quot;)) &#123; try &#123; body = gunzip(body); &#125; catch (IOException e) &#123; metrics.incrementMessagesDropped(); result.set(ResponseEntity.badRequest().body(&quot;Cannot gunzip spans: &quot; + e.getMessage() + &quot;\\n&quot;)); &#125; &#125; //接收span collector.acceptSpans(body, codec, new Callback&lt;Void&gt;() &#123; @Override public void onSuccess(@Nullable Void value) &#123; result.set(SUCCESS); &#125; @Override public void onError(Throwable t) &#123; String message = t.getMessage() == null ? t.getClass().getSimpleName() : t.getMessage(); result.set(t.getMessage() == null || message.startsWith(&quot;Cannot store&quot;) ? ResponseEntity.status(500).body(message + &quot;\\n&quot;) : ResponseEntity.status(400).body(message + &quot;\\n&quot;)); &#125; &#125;); return result; &#125; //略&#125; collector处理器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public final class Collector &#123; /** Needed to scope this to the correct logging category */ public static Builder builder(Class&lt;?&gt; loggingClass) &#123; return new Builder(Logger.getLogger(checkNotNull(loggingClass, &quot;loggingClass&quot;).getName())); &#125; public static final class Builder &#123; final Logger logger; StorageComponent storage = null; CollectorSampler sampler = CollectorSampler.ALWAYS_SAMPLE; CollectorMetrics metrics = CollectorMetrics.NOOP_METRICS; ... public Collector build() &#123; return new Collector(this); &#125; &#125; final Logger logger; final StorageComponent storage; final CollectorSampler sampler; final CollectorMetrics metrics; Collector(Builder builder) &#123; this.logger = checkNotNull(builder.logger, &quot;logger&quot;); this.storage = checkNotNull(builder.storage, &quot;storage&quot;); this.sampler = builder.sampler == null ? CollectorSampler.ALWAYS_SAMPLE : builder.sampler; this.metrics = builder.metrics == null ? CollectorMetrics.NOOP_METRICS : builder.metrics; &#125; public void acceptSpans(byte[] serializedSpans, Codec codec, Callback&lt;Void&gt; callback) &#123; metrics.incrementBytes(serializedSpans.length);//记录指标 List&lt;Span&gt; spans; try &#123; spans = codec.readSpans(serializedSpans);//字节数组转换成对象 &#125; catch (RuntimeException e) &#123; callback.onError(errorReading(e)); return; &#125; accept(spans, callback);//处理span &#125; ... public void accept(List&lt;Span&gt; spans, Callback&lt;Void&gt; callback) &#123; if (spans.isEmpty()) &#123; callback.onSuccess(null); return; &#125; metrics.incrementSpans(spans.size()); List&lt;Span&gt; sampled = sample(spans); if (sampled.isEmpty()) &#123; callback.onSuccess(null); return; &#125; try &#123; storage.asyncSpanConsumer().accept(sampled, acceptSpansCallback(sampled));//处理 callback.onSuccess(null); &#125; catch (RuntimeException e) &#123; callback.onError(errorStoringSpans(sampled, e)); return; &#125; &#125; //取样 List&lt;Span&gt; sample(List&lt;Span&gt; input) &#123; List&lt;Span&gt; sampled = new ArrayList&lt;&gt;(input.size()); for (Span s : input) &#123; if (sampler.isSampled(s)) sampled.add(s); &#125; int dropped = input.size() - sampled.size(); if (dropped &gt; 0) metrics.incrementSpansDropped(dropped); return sampled; &#125; ... &#125; InMemorySpanStore最终处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** Internally, spans are indexed on 64-bit trace ID */public final class InMemorySpanStore implements SpanStore &#123; private final Multimap&lt;Long, Span&gt; traceIdToSpans = new LinkedListMultimap&lt;&gt;();//traceId+span private final Set&lt;Pair&lt;Long&gt;&gt; traceIdTimeStamps = new TreeSet&lt;&gt;(VALUE_2_DESCENDING);//traceId+timestap private final Multimap&lt;String, Pair&lt;Long&gt;&gt; serviceToTraceIdTimeStamp = new SortedByValue2Descending&lt;&gt;(); private final Multimap&lt;String, String&gt; serviceToSpanNames = new LinkedHashSetMultimap&lt;&gt;();//serviceName+spanName private final boolean strictTraceId; volatile int acceptedSpanCount; // Historical constructor public InMemorySpanStore() &#123; this(new InMemoryStorage.Builder()); &#125; InMemorySpanStore(InMemoryStorage.Builder builder) &#123; this.strictTraceId = builder.strictTraceId; &#125; final StorageAdapters.SpanConsumer spanConsumer = new StorageAdapters.SpanConsumer() &#123; @Override public void accept(List&lt;Span&gt; spans) &#123; for (Span span : spans) &#123; Long timestamp = guessTimestamp(span); Pair&lt;Long&gt; traceIdTimeStamp = Pair.create(span.traceId, timestamp == null ? Long.MIN_VALUE : timestamp); String spanName = span.name; synchronized (InMemorySpanStore.this) &#123; traceIdTimeStamps.add(traceIdTimeStamp); traceIdToSpans.put(span.traceId, span); acceptedSpanCount++; for (String serviceName : span.serviceNames()) &#123; serviceToTraceIdTimeStamp.put(serviceName, traceIdTimeStamp); serviceToSpanNames.put(serviceName, spanName); &#125; &#125; &#125; &#125; @Override public String toString() &#123; return &quot;InMemorySpanConsumer&quot;; &#125; &#125;; ...&#125; 查询trace提供api对应查询为配置的storeage123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103@RestController@RequestMapping(&quot;/api/v1&quot;)@CrossOrigin(&quot;$&#123;zipkin.query.allowed-origins:*&#125;&quot;)public class ZipkinQueryApiV1 &#123; @Autowired @Value(&quot;$&#123;zipkin.query.lookback:86400000&#125;&quot;) int defaultLookback = 86400000; // 1 day in millis /** The Cache-Control max-age (seconds) for /api/v1/services and /api/v1/spans */ @Value(&quot;$&#123;zipkin.query.names-max-age:300&#125;&quot;) int namesMaxAge = 300; // 5 minutes volatile int serviceCount; // used as a threshold to start returning cache-control headers private final StorageComponent storage; @Autowired public ZipkinQueryApiV1(StorageComponent storage) &#123; this.storage = storage; // don&apos;t cache spanStore here as it can cause the app to crash! &#125; @RequestMapping(value = &quot;/dependencies&quot;, method = RequestMethod.GET, produces = APPLICATION_JSON_VALUE) public byte[] getDependencies(@RequestParam(value = &quot;endTs&quot;, required = true) long endTs, @RequestParam(value = &quot;lookback&quot;, required = false) Long lookback) &#123; return Codec.JSON.writeDependencyLinks(storage.spanStore().getDependencies(endTs, lookback != null ? lookback : defaultLookback)); &#125; @RequestMapping(value = &quot;/services&quot;, method = RequestMethod.GET) public ResponseEntity&lt;List&lt;String&gt;&gt; getServiceNames() &#123; List&lt;String&gt; serviceNames = storage.spanStore().getServiceNames(); serviceCount = serviceNames.size(); return maybeCacheNames(serviceNames); &#125; @RequestMapping(value = &quot;/spans&quot;, method = RequestMethod.GET) public ResponseEntity&lt;List&lt;String&gt;&gt; getSpanNames( @RequestParam(value = &quot;serviceName&quot;, required = true) String serviceName) &#123; return maybeCacheNames(storage.spanStore().getSpanNames(serviceName)); &#125; @RequestMapping(value = &quot;/traces&quot;, method = RequestMethod.GET, produces = APPLICATION_JSON_VALUE) public String getTraces( @RequestParam(value = &quot;serviceName&quot;, required = false) String serviceName, @RequestParam(value = &quot;spanName&quot;, defaultValue = &quot;all&quot;) String spanName, @RequestParam(value = &quot;annotationQuery&quot;, required = false) String annotationQuery, @RequestParam(value = &quot;minDuration&quot;, required = false) Long minDuration, @RequestParam(value = &quot;maxDuration&quot;, required = false) Long maxDuration, @RequestParam(value = &quot;endTs&quot;, required = false) Long endTs, @RequestParam(value = &quot;lookback&quot;, required = false) Long lookback, @RequestParam(value = &quot;limit&quot;, required = false) Integer limit) &#123; QueryRequest queryRequest = QueryRequest.builder() .serviceName(serviceName) .spanName(spanName) .parseAnnotationQuery(annotationQuery) .minDuration(minDuration) .maxDuration(maxDuration) .endTs(endTs) .lookback(lookback != null ? lookback : defaultLookback) .limit(limit).build(); return new String(Codec.JSON.writeTraces(storage.spanStore().getTraces(queryRequest)), UTF_8); &#125; @RequestMapping(value = &quot;/trace/&#123;traceIdHex&#125;&quot;, method = RequestMethod.GET, produces = APPLICATION_JSON_VALUE) public String getTrace(@PathVariable String traceIdHex, WebRequest request) &#123; long traceIdHigh = traceIdHex.length() == 32 ? lowerHexToUnsignedLong(traceIdHex, 0) : 0L; long traceIdLow = lowerHexToUnsignedLong(traceIdHex); String[] raw = request.getParameterValues(&quot;raw&quot;); // RequestParam doesn&apos;t work for param w/o value List&lt;Span&gt; trace = raw != null ? storage.spanStore().getRawTrace(traceIdHigh, traceIdLow) : storage.spanStore().getTrace(traceIdHigh, traceIdLow); if (trace == null) &#123; throw new TraceNotFoundException(traceIdHex, traceIdHigh, traceIdLow); &#125; return new String(Codec.JSON.writeSpans(trace), UTF_8); &#125; @ExceptionHandler(TraceNotFoundException.class) @ResponseStatus(HttpStatus.NOT_FOUND) public void notFound() &#123; &#125; static class TraceNotFoundException extends RuntimeException &#123; public TraceNotFoundException(String traceIdHex, Long traceIdHigh, long traceId) &#123; super(String.format(&quot;Cannot find trace for id=%s, parsed value=%s&quot;, traceIdHex, traceIdHigh != null ? traceIdHigh + &quot;,&quot; + traceId : traceId)); &#125; &#125; /** * We cache names if there are more than 3 services. This helps people getting started: if we * cache empty results, users have more questions. We assume caching becomes a concern when zipkin * is in active use, and active use usually implies more than 3 services. */ ResponseEntity&lt;List&lt;String&gt;&gt; maybeCacheNames(List&lt;String&gt; names) &#123; ResponseEntity.BodyBuilder response = ResponseEntity.ok(); if (serviceCount &gt; 3) &#123; response.cacheControl(CacheControl.maxAge(namesMaxAge, TimeUnit.SECONDS).mustRevalidate()); &#125; return response.body(names); &#125;&#125; PS 如果更改了存储的类型，默认会进行直接切换，比如storage.type=elasticsearch,基于springboot的autoconfigure原则，ZipkinElasticsearchHttpStorageAutoConfiguration会执行，同时条件成立会直接创建elasticsearchStoreage 当前只查看了inMemory的流程如果有兴趣其他流程可以自己去看 贴下流程图 zipkin-server接收插入请求-inMemory zipkin-server接收查询请求-inMemory 项目源码 参考 Zipkin 源码解析","categories":[{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/categories/zipkin/"}],"tags":[{"name":"源码分析","slug":"源码分析","permalink":"https://mykite.github.io/tags/源码分析/"},{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/tags/springboot/"},{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/tags/zipkin/"}]},{"title":"zipkin教程简单介绍及环境搭建（一）","slug":"zipkin教程简单介绍及环境搭建（一）","date":"2017-04-21T09:41:35.747Z","updated":"2017-08-22T09:33:47.538Z","comments":true,"path":"2017/04/21/zipkin教程简单介绍及环境搭建（一）/","link":"","permalink":"https://mykite.github.io/2017/04/21/zipkin教程简单介绍及环境搭建（一）/","excerpt":"","text":"什么是zipkin Zipkin 是 Twitter 的一个开源项目，允许开发者收集 Twitter 各个服务上的监控数据，并提供查询接口 为什么要使用zipkin 随着业务发展，系统拆分导致系统调用链路愈发复杂一个前端请求可能最终需要调用很多次后端服务才能完成，当整个请求变慢或不可用时，我们是无法得知该请求是由某个或某些后端服务引起的，这时就需要解决如何快读定位服务故障点，以对症下药。于是就有了分布式系统调用跟踪的诞生。而zipkin就是开源分布式系统调用跟踪的佼佼者 zipkin基于google-Dapper的论文有兴趣的可以看下 google-Dapper zipkin体系介绍zipkin架构包含组件 collector 收集器 storage 存储 api 查询api ui 界面 zipkin存储 zipkin存储默认使用inMemory 支持存储模式 inMemory mysql Cassandra Elasticsearch ZipKin数据模型 Trace：一组代表一次用户请求所包含的spans，其中根span只有一个。 Span： 一组代表一次HTTP/RPC请求所包含的annotations。 annotation：包括一个值，时间戳，主机名(留痕迹)。 几个时间 cs：客户端发起请求，标志Span的开始 sr：服务端接收到请求，并开始处理内部事务，其中sr - cs则为网络延迟和时钟抖动 ss：服务端处理完请求，返回响应内容，其中ss - sr则为服务端处理请求耗时 cr：客户端接收到服务端响应内容，标志着Span的结束，其中cr - ss则为网络延迟和时钟抖动 搭建zipkin下载文件 zipkin last jar 下载地址 zipkin-github 启动zipkinjava -jar zipkin-server-1.22.1-exec.jar 使用elasticsearch-5.3.0作为存储启动zipkin链接 elasticsearch-5.3.0下载 github解压elasticsearch-5.3.0运行启动完成界面启动zipkin使用elasticsearch123java -jar zipkin-server-1.22.1-exec.jar --STORAGE_TYPE=elasticsearch --DES_HOSTS=http://localhost:9200zipkin-server-1.22.1-exec.jar采用springboot编写,springboot传入参数使用--key=value.当前为什么使用--STORAGE_TYPE，--DES_HOSTS由配置文件里面决定 zipkin 控制台zipkin 明细zipkin 依赖 springboot+apache-httpclient使用zipkin项目结构 pom.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.kite.zipkin&lt;/groupId&gt; &lt;artifactId&gt;zipkin-demo-server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;zipkin-demo-server&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- brave core --&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt; &lt;artifactId&gt;brave-core&lt;/artifactId&gt; &lt;version&gt;3.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt; &lt;artifactId&gt;brave-spancollector-http&lt;/artifactId&gt; &lt;version&gt;3.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt; &lt;artifactId&gt;brave-web-servlet-filter&lt;/artifactId&gt; &lt;version&gt;3.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.brave&lt;/groupId&gt; &lt;artifactId&gt;brave-apache-http-interceptors&lt;/artifactId&gt; &lt;version&gt;3.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; ZipkinConfig123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.kite.zipkin.config;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import com.github.kristofa.brave.Brave;import com.github.kristofa.brave.Brave.Builder;import com.github.kristofa.brave.EmptySpanCollectorMetricsHandler;import com.github.kristofa.brave.Sampler;import com.github.kristofa.brave.SpanCollector;import com.github.kristofa.brave.http.DefaultSpanNameProvider;import com.github.kristofa.brave.http.HttpSpanCollector;import com.github.kristofa.brave.http.HttpSpanCollector.Config;import com.github.kristofa.brave.httpclient.BraveHttpRequestInterceptor;import com.github.kristofa.brave.httpclient.BraveHttpResponseInterceptor;import com.github.kristofa.brave.servlet.BraveServletFilter;@Configurationpublic class ZipkinConfig &#123; //span（一次请求信息或者一次链路调用）信息收集器 @Bean public SpanCollector spanCollector() &#123; Config config = HttpSpanCollector.Config.builder() .compressionEnabled(false)// 默认false，span在transport之前是否会被gzipped .connectTimeout(5000) .flushInterval(1) .readTimeout(6000) .build(); return HttpSpanCollector.create(&quot;http://localhost:9411&quot;, config, new EmptySpanCollectorMetricsHandler()); &#125; //作为各调用链路，只需要负责将指定格式的数据发送给zipkin @Bean public Brave brave(SpanCollector spanCollector)&#123; Builder builder = new Builder(&quot;service1&quot;);//指定serviceName builder.spanCollector(spanCollector); builder.traceSampler(Sampler.create(1));//采集率 return builder.build(); &#125; //设置server的（服务端收到请求和服务端完成处理，并将结果发送给客户端）过滤器 @Bean public BraveServletFilter braveServletFilter(Brave brave) &#123; BraveServletFilter filter = new BraveServletFilter(brave.serverRequestInterceptor(), brave.serverResponseInterceptor(), new DefaultSpanNameProvider()); return filter; &#125; //设置client的（发起请求和获取到服务端返回信息）拦截器 @Bean public CloseableHttpClient okHttpClient(Brave brave)&#123; CloseableHttpClient httpclient = HttpClients.custom() .addInterceptorFirst(new BraveHttpRequestInterceptor(brave.clientRequestInterceptor(), new DefaultSpanNameProvider())) .addInterceptorFirst(new BraveHttpResponseInterceptor(brave.clientResponseInterceptor())) .build(); return httpclient; &#125; &#125; ZipkinBraveController12345678910111213141516171819202122232425262728package com.kite.zipkin.controller;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.util.EntityUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class ZipkinBraveController &#123; @Autowired private CloseableHttpClient okHttpClient; @GetMapping(&quot;/service1&quot;) public String myboot() throws Exception &#123; Thread.sleep(100);//100ms HttpGet get = new HttpGet(&quot;http://localhost:81/test&quot;); CloseableHttpResponse execute = okHttpClient.execute(get); /* * 1、执行execute()的前后，会执行相应的拦截器（cs,cr） * 2、请求在被调用方执行的前后，也会执行相应的拦截器（sr,ss） */ return EntityUtils.toString(execute.getEntity(), &quot;utf-8&quot;); &#125;&#125; Application启动类1234567891011package com.kite.zipkin;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; ====================== zipkin-demo-server-2 serviceName修改 123456public Brave brave(SpanCollector spanCollector)&#123; Builder builder = new Builder(&quot;service2&quot;);//指定serviceName builder.spanCollector(spanCollector); builder.traceSampler(Sampler.create(1));//采集率 return builder.build(); &#125; controller修改1234567891011121314151617181920212223242526272829303132package com.kite.zipkin.controller;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.util.EntityUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class ZipkinBraveController &#123; @Autowired private CloseableHttpClient httpClient; @GetMapping(&quot;/test&quot;) public String myboot() throws Exception &#123; Thread.sleep(200);//100ms HttpGet get1 = new HttpGet(&quot;http://localhost:82/test&quot;); CloseableHttpResponse execute1 = httpClient.execute(get1); /* * 1、执行execute()的前后，会执行相应的拦截器（cs,cr） * 2、请求在被调用方执行的前后，也会执行相应的拦截器（sr,ss） */ HttpGet get2 = new HttpGet(&quot;http://localhost:83/test&quot;); CloseableHttpResponse execute2 = httpClient.execute(get2); return EntityUtils.toString(execute1.getEntity(), &quot;utf-8&quot;) + &quot;-&quot; +EntityUtils.toString(execute2.getEntity(), &quot;utf-8&quot;); &#125;&#125; zipkin-demo-server-3 serviceName修改1234567@Bean public Brave brave(SpanCollector spanCollector)&#123; Builder builder = new Builder(&quot;service3&quot;);//指定serviceName builder.spanCollector(spanCollector); builder.traceSampler(Sampler.create(1));//采集率 return builder.build(); &#125; controller修改123456789101112131415package com.kite.zipkin.controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class ZipkinBraveController &#123; @GetMapping(&quot;/test&quot;) public String myboot() throws Exception &#123; Thread.sleep(100);//100ms return &quot;service3&quot;; &#125;&#125; zipkin-demo-server-4 serviceName修改 1234567@Bean public Brave brave(SpanCollector spanCollector)&#123; Builder builder = new Builder(&quot;service4&quot;);//指定serviceName builder.spanCollector(spanCollector); builder.traceSampler(Sampler.create(1));//采集率 return builder.build(); &#125; controller修改 123456789101112131415package com.kite.zipkin.controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class ZipkinBraveController &#123; @GetMapping(&quot;/test&quot;) public String myboot() throws Exception &#123; Thread.sleep(100);//100ms return &quot;service3&quot;; &#125;&#125; 关注点point zipkin 运行需要jdk8 pinpoint 可以关注下，同样是分布式链路追踪系统(ps:搭建环境1天下来，trace一直不能生成。。有谁碰到过这个问题可以回复下 0.0)参考 分布式系统调用跟踪实践 Brave接入ZipKin实现调用链跟踪【上】 Architecture","categories":[{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/categories/zipkin/"}],"tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://mykite.github.io/tags/环境搭建/"},{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/tags/springboot/"},{"name":"zipkin","slug":"zipkin","permalink":"https://mykite.github.io/tags/zipkin/"},{"name":"教程","slug":"教程","permalink":"https://mykite.github.io/tags/教程/"}]},{"title":"通过google maps获取国家","slug":"通过google maps 获取国家","date":"2017-04-19T09:16:02.653Z","updated":"2017-05-08T10:10:33.000Z","comments":true,"path":"2017/04/19/通过google maps 获取国家/","link":"","permalink":"https://mykite.github.io/2017/04/19/通过google maps 获取国家/","excerpt":"","text":"需求 根据国家过去相关报警电话, 页面为h5 解决方案 最开始直接想到的是通过百度地图api来进行，实现不过查看官方文档来看，满足不了要求，不能获取到国家级别的数据 百度地图api官方 JavaScript api demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1.0, user-scalable=no&quot; /&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;title&gt;Hello, World&lt;/title&gt; &lt;style type=&quot;text/css&quot;&gt; html &#123; height: 100% &#125; body &#123; height: 100%; margin: 0px; padding: 0px &#125; #container &#123; height: 100% &#125; &lt;/style&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://api.map.baidu.com/api?v=2.0&amp;ak=8a9c8c9b61196a1b5be23217fc94a489&quot;&gt; //v2.0版本的引用方式：src=&quot;http://api.map.baidu.com/api?v=2.0&amp;ak=您的密钥&quot;//v1.4版本及以前版本的引用方式：src=&quot;http://api.map.baidu.com/api?v=1.4&amp;key=您的密钥&amp;callback=initialize&quot; &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;container&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; //var point = new BMap.Point(116.404, 39.915); // 创建点坐标 //map.centerAndZoom(point, 15); // 初始化地图，设置中心点坐标和地图级别 var geolocation = new BMap.Geolocation(); var boundary = new BMap.Boundary(); boundary.get(&quot;上海&quot;, function(data) &#123; console.info(data); &#125;) geolocation.getCurrentPosition(function (r) &#123; if (this.getStatus() == BMAP_STATUS_SUCCESS) &#123; console.info(&apos;您的位置：&apos; + r.point.lng + &apos;,&apos; + r.point.lat); // 创建地理编码实例 var myGeo = new BMap.Geocoder(); myGeo.getLocation(new BMap.Point(r.point.lng, r.point.lat), function (result) &#123; console.info(result); if (result) &#123; console.info(result.address); &#125; &#125;); &#125; else &#123; alert(&apos;failed&apos; + this.getStatus()); &#125; &#125;, &#123; enableHighAccuracy: true &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 最高只能获取到province省份，不能获取到国家 高德地图api官方JavaScript api同样不能拿到 国家，demo 略 h5api&amp;google maps api HTML5 Geolocation（地理定位）用于定位用户的位置。是html5的主要特性之一 demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;test&lt;/title&gt;&lt;/head&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;jquery-1.10.1.min.js&quot;&gt;&lt;/script&gt;&lt;body&gt; &lt;script&gt; if (navigator.geolocation) &#123; // getCurrentPosition支持三个参数 // getSuccess是执行成功的回调函数 // getError是失败的回调函数 // getOptions是一个对象，用于设置getCurrentPosition的参数 // 后两个不是必要参数 var getOptions = &#123; //是否使用高精度设备，如GPS。默认是true enableHighAccuracy : true, //超时时间，单位毫秒，默认为0 timeout : 5000, //使用设置时间内的缓存数据，单位毫秒 //默认为0，即始终请求新数据 //如设为Infinity，则始终使用缓存数据 maximumAge : 0 &#125;; //成功回调 function getSuccess(position) &#123; alert(position); // getCurrentPosition执行成功后，会把getSuccess传一个position对象 // position有两个属性，coords和timeStamp // timeStamp表示地理数据创建的时间？？？？？？ // coords是一个对象，包含了地理位置数据 console.info(position.timeStamp); // 估算的纬度 console.info(position.coords.latitude); // 估算的经度 console.info(position.coords.longitude); alert(&quot;当前位置:&quot; + position.coords.latitude + &quot;,&quot; + position.coords.longitude); getCounty(&#123;&apos;latitude&apos;:position.coords.latitude, &apos;longitude&apos;:position.coords.longitude&#125;); // 估算的高度 (以米为单位的海拔值) console.info(position.coords.altitude); // 所得经度和纬度的估算精度，以米为单位 console.info(position.coords.accuracy); // 所得高度的估算精度，以米为单位 console.info(position.coords.altitudeAccuracy); // 宿主设备的当前移动方向，以度为单位，相对于正北方向顺时针方向计算 console.info(position.coords.heading); // 设备的当前对地速度，以米/秒为单位 console.info(position.coords.speed); // 除上述结果外，Firefox还提供了另外一个属性address if (position.address) &#123; //通过address，可以获得国家、省份、城市 console.info(position.address.country); console.info(position.address.province); console.info(position.address.city); &#125; &#125; //获取国家 根据经纬度获取国家 function getCounty(data) &#123; var url = &quot;https://maps.googleapis.com/maps/api/geocode/json?latlng=&quot; + data.latitude + &quot;,&quot; + data.longitude + &quot;&amp;sensor=false&amp;language=CN&quot;; $.post(url, function(data)&#123; console.info(data); if(data.status == &apos;OK&apos;) &#123; var results = data.results; for (var i=0; i &lt; results[0].address_components.length; i++) &#123; for (var j=0; j &lt; results[0].address_components[i].types.length; j++) &#123; if (results[0].address_components[i].types[j] == &quot;country&quot;) &#123; country = results[0].address_components[i]; console.log(country.long_name) alert(country.long_name) console.log(country.short_name) alert(country.short_name) &#125; &#125; &#125; &#125; else &#123; alert(&apos;定位失败&apos;) &#125; &#125;); &#125; //失败回调 function getError(error) &#123; // 执行失败的回调函数，会接受一个error对象作为参数 // error拥有一个code属性和三个常量属性TIMEOUT、PERMISSION_DENIED、POSITION_UNAVAILABLE // 执行失败时，code属性会指向三个常量中的一个，从而指明错误原因 alert(error); switch (error.code) &#123; case error.TIMEOUT: alert(&quot;超时&quot;) console.info(&apos;超时&apos;); break; case error.PERMISSION_DENIED: alert(&quot;用户拒绝提供地理位置&quot;) console.info(&apos;用户拒绝提供地理位置&apos;); break; case error.POSITION_UNAVAILABLE: alert(&quot;地理位置不可用&quot;); console.info(&apos;地理位置不可用&apos;); break; default: break; &#125; &#125; navigator.geolocation.getCurrentPosition(getSuccess, getError, getOptions); // watchPosition方法一样可以设置三个参数 // 使用方法和getCurrentPosition方法一致，只是执行效果不同。 // getCurrentPosition只执行一次 // watchPosition只要设备位置发生变化，就会执行 var watcher_id = navigator.geolocation.watchPosition(getSuccess, getError, getOptions); //clearwatch用于终止watchPosition方法 navigator.geolocation.clearWatch(watcher_id); &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 通过navigator.geolocation获取到坐标位置，然后通过google maps api可以获取到国家(无需google map api key) ps:国内需要翻墙 需要开启手机定位，如果不开启定位要获取可以使用ip段or百度api定位（百度获取到的经纬度不能直接使用需要进行转换，原因为坐标系统不一致） PS 百度&amp;高德没看到支持，希望提供支持(可能没看到，如果有人懂的可以联系下) 代码略挫，因为前端是直接用vue写的，就不需要我来封装了，省事 参考 Javascript学习总结 - html5实现定位地理位置 Get only countries to autocomplete from Google Maps API","categories":[{"name":"前端","slug":"前端","permalink":"https://mykite.github.io/categories/前端/"}],"tags":[{"name":"h5","slug":"h5","permalink":"https://mykite.github.io/tags/h5/"},{"name":"google maps","slug":"google-maps","permalink":"https://mykite.github.io/tags/google-maps/"}]},{"title":"spring-loaded热部署","slug":"spring-loaded热部署","date":"2017-04-17T07:21:48.278Z","updated":"2017-04-17T07:22:30.000Z","comments":true,"path":"2017/04/17/spring-loaded热部署/","link":"","permalink":"https://mykite.github.io/2017/04/17/spring-loaded热部署/","excerpt":"","text":"什么是spring-loaded?spring-loaded是一个对于jvm代理运行时期改变类文件的重载（重新加载），它转换类loadtime让他们服从后重新加载。不像“热代码替换”只允许一次简单的改变JVM运行(例如更改方法体)spring-loaded允许您添加/修改/删除/字段/方法构造函数。注释类型/方法/字段/构造函数也可以修改和可以添加/删除/修改值的枚举类型。 有什么好处? 开发测试阶段：能够在启动后动态更改代码调试,无需重启减少切换debug时间（ps:对于eclipse而言，在debug时期只能做到动态更新方法体不能增加） 对于线上测试发布阶段： 能够在出现问题后直接替换class文件而不重启应用（ps:对于外部提供的服务jar形式同样能做到） 怎么使用?项目地址1https://github.com/spring-projects/spring-loaded 第一步：下载文件1http://repo.spring.io/release/org/springframework/springloaded/1.2.5.RELEASE/springloaded-1.2.5.RELEASE.jar 第二步：配置jvm启动参数eclipse1234567eclipse：run as --&gt; run configurations --&gt; arguments --&gt;&gt; VM arguments-javaagent:E:\\repository\\org\\springframework\\spring-load\\springloaded-1.2.5.RELEASE.jar -noverify -Dspringloaded=verbose详细描述：-javaagent: 配置java代理使用下载后的jar包路径-noverify: 禁用字节码验证-Dspringloaded=verbose 显示springloaded时的详细信息 java命令启动12java -javaagent:E:\\repository\\org\\springframework\\spring-load\\springloaded-1.2.5.RELEASE.jar -noverify Test 类似 java jar包动态替换1.打成runnable Jar2.命令启动：java -javaagent:E:\\repository\\org\\springframework\\spring-load\\springloaded-1.2.5.RELEASE.jar -noverify -Dspringloaded=watchJars=main.jar main.jar 123456789101112131415161718192021222324/** * 类Test.java的实现描述：TODO 类实现描述 * @author Administrator 2016年7月4日 下午4:55:59 */public class Test &#123; public static void main(String[] args) throws InterruptedException &#123; while(true) &#123; try &#123; println(); Thread.sleep(1000); &#125; catch (Throwable e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void println() &#123; System.out.println(&quot;112222221222222&quot;); &#125; &#125; 改变为 123456789101112131415161718192021222324/** * 类Test.java的实现描述：TODO 类实现描述 * @author Administrator 2016年7月4日 下午4:55:59 */public class Test &#123; public static void main(String[] args) throws InterruptedException &#123; while(true) &#123; try &#123; println(); Thread.sleep(1000); &#125; catch (Throwable e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void println() &#123; System.out.println(&quot;test replace jar&quot;); &#125; &#125; 3.重新打包替换12PS：实测在window下无用手上无linux机器待测试","categories":[{"name":"spring","slug":"spring","permalink":"https://mykite.github.io/categories/spring/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://mykite.github.io/tags/spring/"},{"name":"热部署","slug":"热部署","permalink":"https://mykite.github.io/tags/热部署/"}]},{"title":"","slug":"使用nginx搭建https环境","date":"2017-04-17T07:19:40.252Z","updated":"2017-04-17T07:19:40.252Z","comments":true,"path":"2017/04/17/使用nginx搭建https环境/","link":"","permalink":"https://mykite.github.io/2017/04/17/使用nginx搭建https环境/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"spring-cloud入门环境搭建","slug":"spring-cloud入门环境搭建","date":"2017-04-17T07:18:29.826Z","updated":"2017-04-17T07:19:10.000Z","comments":true,"path":"2017/04/17/spring-cloud入门环境搭建/","link":"","permalink":"https://mykite.github.io/2017/04/17/spring-cloud入门环境搭建/","excerpt":"","text":"1.什么是spring-cloudspring-cloud是spring提供的微服务整合开发框架。Spring Cloud 为开发者提供了在分布式系统（如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性 Token、全局锁、决策竞选、分布式会话和集群状态）操作的开发工具。使用 Spring Cloud 开发者可以快速实现上述这些模式。 2.为什么使用spring-cloud 经历过netflix业务考验，国外大规模使用 入门门槛低，国内大批量使用spring 快速搭建 3.spring-cloud快熟搭建入门1. eureka 服务注册组件下载 https://github.com/mykite/eureka-server.git编译后直接运行即可，或 mvn clean install 后直接运行jar包后访问部署后： 2. configServer对配置的集中管理，使用svn or git https://github.com/mykite/configserver.git 编译后直接运行即可，或 mvn clean install 后直接运行jar包后访问 123456789101112131415161718192021使用方式在configserver中配置的spring: cloud: config: server: git: uri: https://github.com/mykite/config-repostory提交到test分支文件hell-server.yml文件内容：test.name: kite访问：http://localhost:8888/hello-server/profiles/test会访问当前配置github上的test分支下的hello-server.yml(or properties文件)对应应用中的配置spring: cloud: config: uri: http://localhost:8888 label: test可以实现注入 3. ribbonribbon用以实现负载均衡；实现软负载均衡，核心有三点： 服务发现，发现依赖服务的列表 服务选择规则，在多个服务中如何选择一个有效服务 服务监听，检测失效的服务，高效剔除失效服务 服务选择规则,其中包括： 简单轮询负载均衡 加权响应时间负载均衡 区域感知轮询负载均衡 随机负载均衡 4. hystrix断路器 5. zuul类似nginx，提供反向代理的功能 项目搭建项目结构springcloud-server 提供的服务springcloud-client 通过feginClient调用服务springcloud-feginclient 通过feginClient调用serverspringcloud-parent maven父项目 parentpom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.kite.test&lt;/groupId&gt; &lt;artifactId&gt;springcloud-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;springcloud-parent&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;modules&gt; &lt;module&gt;../springcloud-client&lt;/module&gt; &lt;module&gt;../springcloud-server&lt;/module&gt; &lt;module&gt;../springcloud-feginclient&lt;/module&gt; &lt;/modules&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Brixton.SR4&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; serverpom.xml 123456789101112131415&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springcloud-client&lt;/name&gt; &lt;artifactId&gt;springcloud-server&lt;/artifactId&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;parent&gt; &lt;groupId&gt;com.kite.test&lt;/groupId&gt; &lt;artifactId&gt;springcloud-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/parent&gt;&lt;/project&gt; 提供的服务 123456789101112131415161718192021222324252627package com.kite.test.springcloud.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RestController;/** * * 类HelloController.java的实现描述：暴露对外服务 * @author pengliang 2016年8月8日 下午4:23:14 */@RestControllerpublic class HelloController &#123; /** * rest 服务用来测试 * --@requestParam url?xxx=name * --requestBody 认定为json传输解析 url?&#123;xxx=name&#125; * @param name * @return */ @RequestMapping(value = &quot;/hello&quot;, method = RequestMethod.GET) public String hello(String name) &#123; return &quot;&#123;hello: &apos;&quot; + name + &quot;&apos;&#125;&quot;; &#125; &#125; 启动类 123456789101112131415161718package com.kite.test.springcloud;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;//springBoot 作为主启动类@SpringBootApplication@EnableDiscoveryClient@EnableCircuitBreaker public class ServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ServerApplication.class, args); &#125;&#125; feginClientpom.xml 12345678910111213141516171819&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springcloud-feginclient&lt;/name&gt; &lt;artifactId&gt;springcloud-feginclient&lt;/artifactId&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;parent&gt; &lt;groupId&gt;com.kite.test&lt;/groupId&gt; &lt;artifactId&gt;springcloud-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt;&lt;/project&gt; feginClient提供接口 12345678910111213141516171819package com.kite.test.springcloud.feginclient;import org.springframework.cloud.netflix.feign.FeignClient;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;/** * feginClient接口 * 类HelloFeginClient.java的实现描述：通过feginClient自动调用 * @author pengliang 2016年8月8日 下午4:25:36 */@FeignClient(value=&quot;HelloServer&quot;) //对应到的server端的spring.application.namepublic interface HelloFeginClient &#123; @RequestMapping(value = &quot;/hello&quot;, method=RequestMethod.POST) public String hello(@RequestParam(name=&quot;name&quot;) String name);&#125; clietpom.xml 12345678910111213141516171819202122&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springcloud-client&lt;/name&gt; &lt;artifactId&gt;springcloud-client&lt;/artifactId&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;parent&gt; &lt;groupId&gt;com.kite.test&lt;/groupId&gt; &lt;artifactId&gt;springcloud-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.kite.test&lt;/groupId&gt; &lt;artifactId&gt;springcloud-feginclient&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; client 调用服务类 1234567891011121314151617181920212223242526272829package com.kite.test.springcloud.client.controller;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RestController;import com.kite.test.springcloud.feginclient.HelloFeginClient;/** * 调用测试 * 类CallHelloController.java的实现描述：调用feginClient测试 * @author pengliang 2016年8月8日 下午4:42:14 */@RestControllerpublic class CallHelloController &#123; private Logger log = LoggerFactory.getLogger(CallHelloController.class); @Autowired private HelloFeginClient helloFeginClient; @RequestMapping(value=&quot;/hello&quot;, method = RequestMethod.GET) public String hello(String name) &#123; log.info(&quot;call hello parameter:&#123;&#125;&quot;, name); return helloFeginClient.hello(name); &#125;&#125; client 启动类 1234567891011121314151617package com.kite.test.springcloud.client;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.feign.EnableFeignClients;@SpringBootApplication@EnableDiscoveryClient@EnableFeignClients(basePackages = &quot;com.kite.test&quot;)@EnableCircuitBreakerpublic class ClientApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ClientApplication.class, args); &#125;&#125; 调用流程图 应用实例：在具体的微服务用力中我们一般采用json来作为数据传输格式，通过feginClient来对服务调用来做一层封装hystrix在对feginClient调用时对依赖失败做隔离，ribbon做负载均衡（使用feginClient时已经默认集成ribbon） 项目源码 https://github.com/mykite/springcloud-test-compoments.git","categories":[{"name":"spring-cloud","slug":"spring-cloud","permalink":"https://mykite.github.io/categories/spring-cloud/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://mykite.github.io/tags/spring/"},{"name":"spring-cloud","slug":"spring-cloud","permalink":"https://mykite.github.io/tags/spring-cloud/"},{"name":"微服务","slug":"微服务","permalink":"https://mykite.github.io/tags/微服务/"}]},{"title":"搭建redis集群环境","slug":"搭建redis集群环境","date":"2017-04-17T07:10:23.246Z","updated":"2017-04-17T07:11:07.000Z","comments":true,"path":"2017/04/17/搭建redis集群环境/","link":"","permalink":"https://mykite.github.io/2017/04/17/搭建redis集群环境/","excerpt":"","text":"1伪集群，单机上搭建，没多的机器 说明123Redis3.0版本之后支持Cluster当前版本3.2.8redis集群搭建最少6个节点其中3个主节点 修改配置1234567891011121314151617接上文redis环境搭建redis1配置打开集群配置port 7001cluster-enabled yescluster-config-file nodes-7001.confcluster-node-timeout 5000redis2配置port 7002cluster-enabled yescluster-config-file nodes-7002.confcluster-node-timeout 5000redis3配置port 7003cluster-enabled yescluster-config-file nodes-7003.confcluster-node-timeout 5000 启动redis123456cd redis1./redis-server redis.confcd redis2./redis-server redis.confcd redis3./redis-server redis.conf 查看服务是否正常1ps -ef|grep redis 查看redis进程 创建集群1官方有提供工具redis-trib.rb，采用ruby编写，so安装ruby 安装ruby1yum -y install ruby ruby-devel rubygems rpm-build 通过ruby命令工具安装redis1gem install redis 通过redis-trib.rb 创建集群查看命令参数123456789101112131415161718192021222324252627282930313233343536[root@vultr src]# ./redis-trib.rb Usage: redis-trib &lt;command&gt; &lt;options&gt; &lt;arguments ...&gt; set-timeout host:port milliseconds reshard host:port --pipeline &lt;arg&gt; --to &lt;arg&gt; --yes --slots &lt;arg&gt; --from &lt;arg&gt; --timeout &lt;arg&gt; del-node host:port node_id add-node new_host:new_port existing_host:existing_port --slave --master-id &lt;arg&gt; fix host:port --timeout &lt;arg&gt; help (show this help) rebalance host:port --pipeline &lt;arg&gt; --simulate --auto-weights --use-empty-masters --weight &lt;arg&gt; --timeout &lt;arg&gt; --threshold &lt;arg&gt; check host:port info host:port create host1:port1 ... hostN:portN --replicas &lt;arg&gt; import host:port --replace --copy --from &lt;arg&gt; call host:port command arg arg .. argFor check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster. 创建集群，出错12345678./redis-trib.rb create --replicas 1 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 *** ERROR: Invalid configuration for cluster creation.*** Redis Cluster requires at least 3 master nodes.*** This is not possible with 3 nodes and 1 replicas per node.*** At least 6 nodes are required.至少需要6个节点 开始创建节点 重新创建redis集群 测试1234567891011121314151.通过redis-cli链接redis redis-cli -c -p 7001[root@vultr bin]# redis-cli -c -p 7001127.0.0.1:7001&gt; set name kite-&gt; Redirected to slot [5798] located at 127.0.0.1:7002OK127.0.0.1:7002&gt; get name&quot;kite&quot;127.0.0.1:7002&gt; exit[root@vultr bin]# redis-cli -c -p 7003127.0.0.1:7003&gt; get name-&gt; Redirected to slot [5798] located at 127.0.0.1:7002&quot;kite&quot;127.0.0.1:7002&gt; 已经进行同步 参考Redis 3.2.1集群搭建","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://mykite.github.io/categories/环境搭建/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://mykite.github.io/tags/redis/"}]},{"title":"springboot使用redis","slug":"springboot使用redis","date":"2017-04-17T07:08:48.946Z","updated":"2017-04-17T07:09:21.000Z","comments":true,"path":"2017/04/17/springboot使用redis/","link":"","permalink":"https://mykite.github.io/2017/04/17/springboot使用redis/","excerpt":"","text":"项目创建通过maven创建项目 添加依赖pom.xml12345678910111213141516171819202122232425262728293031323334353637&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.kite.springboot&lt;/groupId&gt; &lt;artifactId&gt;redis&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;redis&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.7&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 添加配置application.yml123456789spring.redis: database: 0 host: 45.32.112.158 port: 7001 pool: max-idle: 8 min-idle: 0 max-active: 8 max-wait: -1 测试使用出现问题12345678910111213141516171819202122232425262728293031package com.kite.springboot.redis;import java.util.Set;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RestController;/** * Hello world! * */@SpringBootApplication@RestControllerpublic class App &#123; @Autowired StringRedisTemplate stringRedisTemplate; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125; @GetMapping(&quot;/test&quot;) public String test() &#123; Set&lt;String&gt; keys = stringRedisTemplate.keys(&quot;aa&quot;); System.out.println(keys); return &quot;ok&quot;; &#125;&#125; 12345connet refersh链接不上，注释掉bind(测试使用，生产请勿关闭)DENIED Redis is running in protected mode because protected mode is enabled保护模式开启，修改为关闭(测试使用，生产请勿关闭)正常连接输出 注释bind后显示*","categories":[{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/categories/springboot/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://mykite.github.io/tags/redis/"},{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/tags/springboot/"}]},{"title":"redis集群环境搭建","slug":"redis环境搭建","date":"2017-04-17T07:07:54.530Z","updated":"2017-04-17T07:08:28.000Z","comments":true,"path":"2017/04/17/redis环境搭建/","link":"","permalink":"https://mykite.github.io/2017/04/17/redis环境搭建/","excerpt":"","text":"下载redis下载地址 下载redis-stable.tar.gz版本,当前为3.2版本 安装123tar -zxvf redis-stable.tar.gzcd redis-stablemake PREFIX=/usr/local/redis1 &amp; install (指定安装目录) PS:安装的坑1234查看gcc版本gcc -v 不能低于4.2升级gcc yum update gcc 会升级到4.4.7 配置redis 配置详解 NETWORK 网络配置项 123456bind 127.0.0.1(绑定的主机地址)protected-mode yes(是否开启网络保护模式，默认开启)port 6379(redis服务端口，默认6379)tcp-backlog 511timeout 0(该参数表示当某一个客户端连接上来并闲置timeout（单位秒）的时间后，Redis服务端就主动关闭这个客户端连接。该配置参数的默认值为0，表示关闭这个功能。)tcp-keepalive 300(客户端TCP连接的健康性检查，如果不设置为0就表示Redis服务端会定时发送SO_KEEPALIVE心跳机制检测客户端的反馈情况。该配置的默认值为300秒，既是300秒检测一次。健康性检查的好处是，在客户端异常关闭的情况下，Redis服务端可以发现这个问题，并主动关闭对端通道。这个参数建议开启) GENERAL 一般配置项 123456789daemonize no(当为yes的时候，以守护进程的模式运行。该参数的默认值为 no，主要目的是为了在测试环境下调试方便；当运行在生产环境时，可以将这个选项配置为yes)supervised nopidfile /var/run/redis_6379.pidloglevel noticelogfile &quot;&quot;syslog-enabled nosyslog-ident redissyslog-facility local0 (指定syslog工具。必须是用户或LOCAL0-LOCAL7之间,默认没有打开)databases 16 SNAPSHOTTING 快照配置项 12345678save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename dump.rdbdir ./ REPLICATION 复制，高可用配置项 1234slaveof &lt;masterip&gt; &lt;masterport&gt;masterauth &lt;master-password&gt;slave-serve-stale-data yesslave-read-only yes SECURITY 安全配置项 LIMITS 资源限制配置项 APPEND ONLY MODE 附加配置 LUA SCRIPTING lua脚本配置项 REDIS CLUSTER 集群配置项 SLOW LOG 日志配置项 LATENCY MONITOR 监控配置项 EVENT NOTIFICATION 事件通知配置项 ADVANCED CONFIG 高级配置项 太懒写不下去了需要具体看的请查看redis.conf文件复制地址123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107110811091110111111121113111411151116111711181119112011211122# Redis configuration file example.## Redis配置文件示例## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## 注意：为了读取配置文件，Redis必须把配置文件路径作为第一参数：## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 单位注意：当需要指定内存大小的时候，需要指定1k 5GB 4m等类似这种的单位：## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.## 单位对大小写不敏感，所以 1GB 1Gb 1gb是一样的################################## INCLUDES(引入) #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## 在这里引入一个或更多个配置文件。假如你有一个标准模板供所有Redis使用，但是对每个单独的Redis## 又有单独的配置。在这里引入其他的文件，是非常有用的# Notice option &quot;include&quot; won&apos;t be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you&apos;d better put includes# at the beginning of this file to avoid overwriting config change at runtime.### 请注意：include是不能被来自admin或Redis Sentinel的CONFIG REWRITE命令所改写的。所以Redis总是## 最后一条加工线作为配置的指令值，你最好把include文件操作放在前面，去避开在运行期间的覆写操作# If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## 相反，如果你想用include去覆写配置项，你最好把它放在后面## include /path/to/local.conf# include /path/to/other.conf################################## NETWORK(网络) ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## 默认情况，如果没有bind配置，Redis服务端会监听外界所有的可用的连接。如果可以的话，最好## 用bind配置一个或多个确定的地址。# Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 lookback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## --警告-- 如果运行Redis的计算机直接暴露于外网，接受所有的接口是非常危险的，将会把实例暴露给网络上## 的每个人。所有，默认情况下，我们注释掉了下面的bind命令，这样强制Redis只监听本机的接口（意思就是## Redis只能来自接受运行着服务端的计算机的客户端连接，注：就是只能本机连接）## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.## 如果你非常肯定你想要你的实例监听所有的接口，请注释掉下面这一行吧# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## 保护模式是一层安全保障，为了避开可以接触到的网络上的Redis实例# When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## 当安全模式会开启，并且1) 服务端没有显示指定任何bind、2)没有配置密码时，## 服务器将只会接受来本地IP的连接和Unix domain sockets# The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.# 默认情况下，安全模式是启用的，如果你想用来自其他主机的客户端连接他，你应该禁止它（下面的两句太啰嗦，没翻译）protected-mode no# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.## 监听端口，如果设为0，Redis将不会监听TCP连接port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.## 此参数确定了TCP连接中已完成队列(完成三次握手之后)的长度， 当然此值必须不大于## Linux系统定义的/proc/sys/net/core/somaxconn值，默认是511，而Linux的默认参数值是128。## 当系统并发量大并且客户端速度缓慢的时候，可以将这二个参数一起参考设定。（不懂意思，从网上复制的）tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## 指定 Unix socket 的路径，如果没指定，Redis将不会监听Unix socket# unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)# 客户端与服务端的连接超时时间，0为如不超市timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.## 周期性的使用SO_KEEPALIVE检测客户端是否还处于健康状态，避免服务器一直阻塞，## 官方建议300秒（从3.2.1版本开始的）tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use &apos;yes&apos; if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.## 默认情况，Redis不会以后台守护进程方式启动，如果你需要设成&quot;yes&quot;，当设置后，## Redis会写一个pid 文件，在/var/run/redis.piddaemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.## 好像是设成开机启动后，系统监控等东东，我对Linux一知半解，不翻译了supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.## 如果没设置后台守护进程，且没指定pidfile，则没有pid文件被创建## 如果设置了后台守护进程，则会创建/var/run/redis.pid## 如果没设置后台守护进程，且指定了pidfile，则以pidfile为准## 创建pid文件是尽力服务行为，意思就是如果没创建成功也无所谓不耽误Redis正常启动和运行pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)# 定义日志级别。# 可以是下面的这些值：# debug (适用于开发或测试阶段)# verbose (比debug少点，但是也不少)# notice (适用于生产环境)# warning (仅仅一些重要的消息被记录)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/null## 指定日志文件位置logfile &quot;&quot;# To enable logging to the system logger, just set &apos;syslog-enabled&apos; to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and &apos;databases&apos;-1## 数据库数量，默认16个（0-15）databases 16################################ SNAPSHOTTING(快照) ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;## 存 DB 到磁盘：## 格式：save &lt;间隔时间（秒）&gt; &lt;写入次数&gt;## 根据给定的时间间隔和写入次数将数据保存到磁盘## 下面的例子的意思是：# 900 秒内如果至少有 1 个 key 的值变化，则保存# 300 秒内如果至少有 10 个 key 的值变化，则保存# 60 秒内如果至少有 10000 个 key 的值变化，则保存# # 注意：你可以注释掉所有的 save 行来停用保存功能。# 也可以直接一个空字符串来实现停用：# save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.# 默认情况下，如果 redis 最后一次的后台保存失败，redis 将停止接受写操作，# 这样以一种强硬的方式让用户知道数据不能正确的持久化到磁盘，# 否则就会没人注意到灾难的发生。## 如果后台保存进程重新启动工作了，redis 也将自动的允许写操作。## 然而你要是安装了靠谱的监控，你可能不希望 redis 这样做，那你就改成 no 好了。stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that&apos;s set to &apos;yes&apos; as it&apos;s almost always a win.# If you want to save some CPU in the saving child set it to &apos;no&apos; but# the dataset will likely be bigger if you have compressible values or keys.# 是否在 dump .rdb 数据库的时候使用 LZF 压缩字符串# 默认都设为 yes# 如果你希望保存子进程节省点 cpu ，你就设置它为 no ，# 不过这个数据集可能就会比较大rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.# 是否校验rdb文件rdbchecksum yes# The filename where to dump the DB# 设置 dump 的文件位置dbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &apos;dbfilename&apos; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.# 工作目录# 例如上面的 dbfilename 只指定了文件名，# 但是它会写入到这个目录下。这个配置项一定是个目录，而不能是文件名。dir ./################################# REPLICATION(主从复制) ################################## Master-Slave replication. Use slaveof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of slaves.# 2) Redis slaves are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition slaves automatically try to reconnect to masters# and resynchronize with them.## slaveof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the slave to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the slave request.## masterauth &lt;master-password&gt;# When a slave loses its connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to &apos;yes&apos; (the default) the slave will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale-data is set to &apos;no&apos; the slave will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO and SLAVEOF.## 当一个 slave 与 master 失去联系，或者复制正在进行的时候，# slave 可能会有两种表现：## 1) 如果为 yes ，slave 仍然会应答客户端请求，但返回的数据可能是过时，# 或者数据可能是空的在第一次同步的时候## 2) 如果为 no ，在你执行除了 info he salveof 之外的其他命令时，# slave 都将返回一个 &quot;SYNC with master in progress&quot; 的错误，slave-serve-stale-data yes# You can configure a slave instance to accept writes or not. Writing against# a slave instance may be useful to store some ephemeral data (because data# written on a slave will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default slaves are read-only.## Note: read only slaves are not designed to be exposed to untrusted clients# on the internet. It&apos;s just a protection layer against misuse of the instance.# Still a read only slave exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only slaves using &apos;rename-command&apos; to shadow all the# administrative / dangerous commands.# 从机是否只读slave-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New slaves and reconnecting slaves that are not able to continue the replication# process just receiving differences, need to do what is called a &quot;full# synchronization&quot;. An RDB file is transmitted from the master to the slaves.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the slaves incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to slave sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more slaves# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new slaves arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple slaves# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.# 无硬盘备份repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the slaves.## This is important since once the transfer starts, it is not possible to serve# new slaves arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more slaves arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.# 备份等待延迟时间repl-diskless-sync-delay 5# Slaves send PINGs to server in a predefined interval. It&apos;s possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of slave.# 2) Master timeout from the point of view of slaves (data, pings).# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60# Disable TCP_NODELAY on the slave socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to slaves. But this can add a delay for# the data to appear on the slave side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the slave side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and slaves are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# slave data when slaves are disconnected for some time, so that when a slave# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the slave missed while# disconnected.## The bigger the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a slave connected.## 设置主从复制容量大小。这个 backlog 是一个用来在 slaves 被断开连接时# 存放 slave 数据的 buffer，所以当一个 slave 想要重新连接，通常不希望全部重新同步，# 只是部分同步就够了，仅仅传递 slave 在断开连接时丢失的这部分数据。# repl-backlog-size 1mb# After a master has no longer connected slaves for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last slave disconnected, for# the backlog buffer to be freed.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The slave priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a slave to promote into a# master if the master is no longer working correctly.## A slave with a low priority number is considered better for promotion, so# for instance if there are three slaves with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the slave as not able to perform the# role of master, so a slave with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.# 当 主机 不能正常工作的时候，Redis Sentinel 会从 slaves 中选出一个新的 master，# 这个值越小，就越会被优先选中，但是如果是 0 ， 那是意味着这个 slave 不可能被选中。## 默认优先级为 100。slave-priority 100# It is possible for a master to stop accepting writes if there are less than# N slaves connected, having a lag less or equal than M seconds.## The N slaves need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the slave, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough slaves# are available, to the specified number of seconds.## For example to require at least 3 slaves with a lag &lt;= 10 seconds use:## min-slaves-to-write 3# min-slaves-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.# 设置密码# requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to slaves may cause problems.################################### LIMITS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error &apos;max number of clients reached&apos;.## 最大连接数，一旦达到最大限制，redis 将关闭所有的新连接# 并发送一个‘max number of clients reached’的错误。# maxclients 10000# Don&apos;t use more memory than the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can&apos;t remove keys according to the policy, or if the policy is# set to &apos;noeviction&apos;, Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU cache, or to set# a hard memory limit for an instance (using the &apos;noeviction&apos; policy).## WARNING: If you have slaves attached to an instance with maxmemory on,# the size of the output buffers needed to feed the slaves are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of slaves is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is &apos;noeviction&apos;).# 如果你设置了这个值，当缓存的数据容量达到这个值， redis 将根据你选择的# eviction 策略来移除一些 keys。## 如果 redis 不能根据策略移除 keys ，或者是策略被设置为 ‘noeviction’，# redis 将开始响应错误给命令，如 set，lpush 等等，# 并继续响应只读的命令，如 get# maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; remove the key with an expire set using an LRU algorithm# allkeys-lru -&gt; remove any key according to the LRU algorithm# volatile-random -&gt; remove a random key with an expire set# allkeys-random -&gt; remove a random key, any key# volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)# noeviction -&gt; don&apos;t expire at all, just return an error on write operations## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## 设置缓存移除策略（上面五个）# maxmemory-policy noeviction# LRU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs a bit more CPU. 3 is very fast but not very accurate.## maxmemory-samples 5############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don&apos;t fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that&apos;s usually the right compromise between# speed and data safety. It&apos;s up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that&apos;s snapshotting),# or on the contrary, use &quot;always&quot; that&apos;s very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it&apos;s possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can&apos;t happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn&apos;t want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.# lua脚本执行时间（毫秒）lua-time-limit 5000################################ REDIS CLUSTER(集群) ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as &quot;mature&quot; we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can&apos;t be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## 启用集群# cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## Redis自动设成的配置文件，保证每个集群不同# cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## 集群节点超时时间(毫秒)# cluster-node-timeout 15000# A slave of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a slave to actually have a exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple slaves able to failover, they exchange messages# in order to try to give an advantage to the slave with the best# replication offset (more data from the master processed).# Slaves will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single slave computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the slave will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a slave will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * slave-validity-factor) + repl-ping-slave-period## So for example if node-timeout is 30 seconds, and the slave-validity-factor# is 10, and assuming a default repl-ping-slave-period of 10 seconds, the# slave will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large slave-validity-factor may allow slaves with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a slave at all.## For maximum availability, it is possible to set the slave-validity-factor# to a value of 0, which means, that slaves will always try to failover the# master regardless of the last time they interacted with the master.# (However they&apos;ll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-slave-validity-factor 10# Cluster slaves are able to migrate to orphaned masters, that are masters# that are left without working slaves. This improves the cluster ability# to resist to failures as otherwise an orphaned master can&apos;t be failed over# in case of failure if it has no working slaves.## Slaves migrate to orphaned masters only if there are still at least a# given number of other working slaves for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a slave# will migrate only if there is at least 1 other working slave for its master# and so forth. It usually reflects the number of slaves you want for every# master in your cluster.## Default is 1 (slaves migrate only if their masters remain with at least# one slave). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.% 记录慢日志slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.# 记录占用内存大的日志slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don&apos;t have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION(事件通知) ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don&apos;t need# this feature and the feature has some overhead. Note that if you don&apos;t# specify at least one of K or E, no events will be delivered.# 客户端可接受的key通知类型，比如想订阅key的过期通知，就设成Ex，E表示key的事件,x表示key过期事件notify-keyspace-events &quot;&quot;############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.# 长度低于设置大小会使用紧凑内存，value大小低于设定大小会使用紧凑内存# 下面list zset设置类似hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don&apos;t start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don&apos;t compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don&apos;t have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can&apos;t consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# slave -&gt; slave clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don&apos;t receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and slave clients, since# subscribers and slaves receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes 具体修改配置1234567进入到刚配置的目录cd /usr/local/redis1目录下不存在redis.conf 文件，去解压目录下的redis-stable下cp redis.conf /usr/local/redis1/bindaemonize 修改为yes 后台运行port 改为7001 方便搭建集群环境bild 去除 启动1./redis-server redis.conf 参考 亲密接触Redis-第一天 架构设计：系统存储（15）——Redis基本概念和安装使用 PS 推荐以上两个博客的其他文章","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://mykite.github.io/categories/环境搭建/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://mykite.github.io/tags/redis/"},{"name":"环境搭建","slug":"环境搭建","permalink":"https://mykite.github.io/tags/环境搭建/"}]},{"title":"使用springboot+springsession实现分布式session以及源码解析","slug":"使用springboot+springsession实现分布式session以及源码解析","date":"2017-04-17T07:06:58.656Z","updated":"2017-04-17T07:07:40.000Z","comments":true,"path":"2017/04/17/使用springboot+springsession实现分布式session以及源码解析/","link":"","permalink":"https://mykite.github.io/2017/04/17/使用springboot+springsession实现分布式session以及源码解析/","excerpt":"","text":"1接上问springboot使用redis springsession是什么 实现分布式session管理 为什么要使用springsessionspring全家桶，不想自己实现分布式session管理可以使用 添加依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置1234@EnableRedisHttpSessionpublic class HttpSessionConfiguration &#123; &#125; 测试 原理分析SpringHttpSession 第一步查看@EnableRedisHttpSession 12345678@Retention(java.lang.annotation.RetentionPolicy.RUNTIME)@Target(&#123; java.lang.annotation.ElementType.TYPE &#125;)@Documented@Import(RedisHttpSessionConfiguration.class)@Configurationpublic @interface EnableRedisHttpSession &#123;...&#125; 进二步查看RedisHttpSessionConfiguration 123456@Configuration@EnableSchedulingpublic class RedisHttpSessionConfiguration extends SpringHttpSessionConfiguration implements EmbeddedValueResolverAware, ImportAware &#123;...&#125; 第三步发现其集成自SpringHttpSessionConfiguration查看 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class SpringHttpSessionConfiguration implements ApplicationContextAware &#123; private CookieHttpSessionStrategy defaultHttpSessionStrategy = new CookieHttpSessionStrategy(); private boolean usesSpringSessionRememberMeServices; private ServletContext servletContext; private CookieSerializer cookieSerializer; private HttpSessionStrategy httpSessionStrategy = this.defaultHttpSessionStrategy; private List&lt;HttpSessionListener&gt; httpSessionListeners = new ArrayList&lt;HttpSessionListener&gt;(); @PostConstruct public void init() &#123; if (this.cookieSerializer != null) &#123; this.defaultHttpSessionStrategy.setCookieSerializer(this.cookieSerializer); &#125; else if (this.usesSpringSessionRememberMeServices) &#123; DefaultCookieSerializer cookieSerializer = new DefaultCookieSerializer(); cookieSerializer.setRememberMeRequestAttribute( SpringSessionRememberMeServices.REMEMBER_ME_LOGIN_ATTR); this.defaultHttpSessionStrategy.setCookieSerializer(cookieSerializer); &#125; &#125; @Bean public SessionEventHttpSessionListenerAdapter sessionEventHttpSessionListenerAdapter() &#123; return new SessionEventHttpSessionListenerAdapter(this.httpSessionListeners); &#125; @Bean public &lt;S extends ExpiringSession&gt; SessionRepositoryFilter&lt;? extends ExpiringSession&gt; springSessionRepositoryFilter( SessionRepository&lt;S&gt; sessionRepository) &#123; SessionRepositoryFilter&lt;S&gt; sessionRepositoryFilter = new SessionRepositoryFilter&lt;S&gt;( sessionRepository); sessionRepositoryFilter.setServletContext(this.servletContext); if (this.httpSessionStrategy instanceof MultiHttpSessionStrategy) &#123; sessionRepositoryFilter.setHttpSessionStrategy( (MultiHttpSessionStrategy) this.httpSessionStrategy); &#125; else &#123; sessionRepositoryFilter.setHttpSessionStrategy(this.httpSessionStrategy); &#125; return sessionRepositoryFilter; &#125; public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; if (ClassUtils.isPresent( &quot;org.springframework.security.web.authentication.RememberMeServices&quot;, null)) &#123; this.usesSpringSessionRememberMeServices = !ObjectUtils .isEmpty(applicationContext .getBeanNamesForType(SpringSessionRememberMeServices.class)); &#125; &#125; @Autowired(required = false) public void setServletContext(ServletContext servletContext) &#123; this.servletContext = servletContext; &#125; @Autowired(required = false) public void setCookieSerializer(CookieSerializer cookieSerializer) &#123; this.cookieSerializer = cookieSerializer; &#125; @Autowired(required = false) public void setHttpSessionStrategy(HttpSessionStrategy httpSessionStrategy) &#123; this.httpSessionStrategy = httpSessionStrategy; &#125; @Autowired(required = false) public void setHttpSessionListeners(List&lt;HttpSessionListener&gt; listeners) &#123; this.httpSessionListeners = listeners; &#125;&#125;发现其session默认的策略是使用defaultHttpSessionStrategy=new CookieHttpSessionStrategy();cookie来实现继续看@Beanpublic &lt;S extends ExpiringSession&gt; SessionRepositoryFilter&lt;? extends ExpiringSession&gt; springSessionRepositoryFilter( SessionRepository&lt;S&gt; sessionRepository) &#123; SessionRepositoryFilter&lt;S&gt; sessionRepositoryFilter = new SessionRepositoryFilter&lt;S&gt;( sessionRepository); sessionRepositoryFilter.setServletContext(this.servletContext); if (this.httpSessionStrategy instanceof MultiHttpSessionStrategy) &#123; sessionRepositoryFilter.setHttpSessionStrategy( (MultiHttpSessionStrategy) this.httpSessionStrategy); &#125; else &#123; sessionRepositoryFilter.setHttpSessionStrategy(this.httpSessionStrategy); &#125; return sessionRepositoryFilter;&#125;传入参数SessionRepository的实现类RedisOperationsSessionRepository在RedisHttpSessionConfiguration被进行创建所以sessionRepository使用的就是RedisOperationsSessionRepository用来做于存储 继续查看SessionRepositoryFilter1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class SessionRepositoryFilter&lt;S extends ExpiringSession&gt; extends OncePerRequestFilter &#123;...&#125;继承自OncePerRequestFilterabstract class OncePerRequestFilter implements Filter &#123; public final void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; 调用doFilterInternal由SessionRepositoryFilter实现 @Overrideprotected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; request.setAttribute(SESSION_REPOSITORY_ATTR, this.sessionRepository); SessionRepositoryRequestWrapper wrappedRequest = new SessionRepositoryRequestWrapper( request, response, this.servletContext); SessionRepositoryResponseWrapper wrappedResponse = new SessionRepositoryResponseWrapper( wrappedRequest, response); HttpServletRequest strategyRequest = this.httpSessionStrategy .wrapRequest(wrappedRequest, wrappedResponse); HttpServletResponse strategyResponse = this.httpSessionStrategy .wrapResponse(wrappedRequest, wrappedResponse); try &#123; filterChain.doFilter(strategyRequest, strategyResponse); &#125; finally &#123; wrappedRequest.commitSession(); &#125;&#125;包装请求，响应对象根据策略处理包装请求对象最后wrappedRequest.commitSession();HttpSessionWrapper wrappedSession = getCurrentSession();if (wrappedSession == null) &#123; if (isInvalidateClientSession()) &#123; SessionRepositoryFilter.this.httpSessionStrategy .onInvalidateSession(this, this.response); &#125;&#125;else &#123; S session = wrappedSession.getSession(); SessionRepositoryFilter.this.sessionRepository.save(session); if (!isRequestedSessionIdValid() || !session.getId().equals(getRequestedSessionId())) &#123; SessionRepositoryFilter.this.httpSessionStrategy.onNewSession(session, this, this.response); &#125;&#125;这就是最终处理，就不做详细解释了 几张图 SessionRepository的实现类 参考springsession","categories":[{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/categories/springboot/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://mykite.github.io/tags/spring/"},{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/tags/springboot/"},{"name":"springsession","slug":"springsession","permalink":"https://mykite.github.io/tags/springsession/"}]},{"title":"docker学习笔记","slug":"docker学习笔记","date":"2017-04-17T07:06:01.951Z","updated":"2017-04-17T07:06:31.000Z","comments":true,"path":"2017/04/17/docker学习笔记/","link":"","permalink":"https://mykite.github.io/2017/04/17/docker学习笔记/","excerpt":"","text":"什么是Docker Docker是使用google公司推出的Go语言进行开发实现，基于Linux内核的cgroup,namespace以及AUFS类的union FS等技术，对进程进行封装隔离属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 Docker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 Docker 技术比虚拟机技术更为轻便、快捷。 Go语言 cgroup namespace AUFS Union FS为什么要使用Docker特点 更高效的利用系统资源 更快速的启动时间 一致的运行环境 持续交付和部署 更轻松的迁移 更轻松的维护和扩展对比传统虚拟机技术 特性 容器 虚拟机 启动 秒级 分钟级 硬盘使用 一般为MB 一般为 GB 性能 接近原生 弱于 系统支持量 单机支持上千个容器 一般几十个 传统虚拟机docker docker基本概念 镜像 容器 仓库 参考Docker — 从入门到实践","categories":[{"name":"docker","slug":"docker","permalink":"https://mykite.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://mykite.github.io/tags/docker/"}]},{"title":"springboot打包docker镜像部署","slug":"springboot打包docker镜像部署","date":"2017-04-17T07:04:09.935Z","updated":"2017-04-17T07:05:28.000Z","comments":true,"path":"2017/04/17/springboot打包docker镜像部署/","link":"","permalink":"https://mykite.github.io/2017/04/17/springboot打包docker镜像部署/","excerpt":"","text":"环境准备机器 vultr一台,centos7 资源下载 jdk8 maven git yum install git docker yum install docker-io 环境搭建jdk,maven12345678910111.解压资源tar -zxvf jdk8.tar.gztar -zxvf apache-maven-3.3.9-bin.tar.gz2.配置环境变量vim /etc/profileexport JAVA_HOME=/root/jdk8export MAVEN_HOME=/root/apache-maven-3.3.9export PATH=$JAVA_HOME/bin:$MAVEN_HOME/bin:$PATH3.资源生效source /etc/profile 项目准备(使用现有项目)pan-search-springbootpom.xml新增docker配置123456789101112131415&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.3&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/imageName&gt; &lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/plugin&gt; 配置解释 imageName镜像名称 dockerDirectory Dockerfile位置 resources 指那些需要和 Dockerfile 放在一起，在构建镜像时使用的文件，一般应用 jar 包需要纳入。本例，只需一个 jar 文件Dockerfile定义 /src/main/docker/Dockerfile1234FROM frolvlad/alpine-oraclejdk8:slimVOLUME /tmpADD docker-spring-boot-1.0.0.jar app.jarENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] 配置解释 VOLUME 指定了临时文件目录为/tmp。其效果是在主机 /var/lib/docker 目录下创建了一个临时文件，并链接到容器的/tmp。改步骤是可选的，如果涉及到文件系统的应用就很有必要了。/tmp目录用来持久化到 Docker 数据文件夹，因为 Spring Boot 使用的内嵌 Tomcat 容器默认使用/tmp作为工作目录 项目的 jar 文件作为 “app.jar” 添加到容器的 ENTRYPOINT 执行项目 app.jar。为了缩短 Tomcat 启动时间，添加一个系统属性指向 “/dev/urandom” 作为 Entropy Source 构建dockerImage12项目根路径下执行mvn package docker:build 运行docker1docker run -p 8080:8080 -t kite/pan-search-springboot 打包上传账号注册register 登陆1docker login 上传1docker push kitesweet/pan-search-springboot 拉取镜像1docker pull kitesweet/pan-search-springboot 常用docker命令123456789101112131415查看docker psdocker ps -a为查看所有的容器，包括已经停止的删除所有容器docker rm $(docker ps -a -q)删除单个容器docker rm &lt;容器名orID&gt;停止、启动、杀死一个容器docker stop &lt;容器名orID&gt;docker start &lt;容器名orID&gt;docker kill &lt;容器名orID&gt;查看所有镜像docker images查看容器日志docker logs -f &lt;容器名orID&gt; 参考 常用docker命令，及一些坑 用 Docker 构建、运行、发布一个 Spring Boot 应用","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://mykite.github.io/categories/环境搭建/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://mykite.github.io/tags/docker/"},{"name":"springboot","slug":"springboot","permalink":"https://mykite.github.io/tags/springboot/"}]},{"title":"jvm 启动参数配置参考","slug":"jvm启动参数配置参考","date":"2017-04-17T07:01:22.687Z","updated":"2017-04-17T07:01:57.000Z","comments":true,"path":"2017/04/17/jvm启动参数配置参考/","link":"","permalink":"https://mykite.github.io/2017/04/17/jvm启动参数配置参考/","excerpt":"","text":"java -jar xxx.jar -Xms1024m -Xmx1024m -XX:NewSize=512m -XX:MaxNewSize=512m -XX:PermSize=256m -XX:MaxPermSize=256m -XX:+UseConcMarkSweepGC -XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection -XX:+CMSParallelRemarkEnabled -XX:+CMSPermGenSweepingEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:+DisableExplicitGC -XX:+UseCompressedOops -XX:+DoEscapeAnalysis -XX:MaxTenuringThreshold=10 -verbose:gc -Xloggc:/alidata1/admin/logs/gc.log -XX:+PrintGCDetails","categories":[{"name":"jvm","slug":"jvm","permalink":"https://mykite.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://mykite.github.io/tags/jvm/"}]},{"title":"java命令","slug":"java命令","date":"2017-04-17T07:00:06.350Z","updated":"2017-04-17T07:00:47.000Z","comments":true,"path":"2017/04/17/java命令/","link":"","permalink":"https://mykite.github.io/2017/04/17/java命令/","excerpt":"","text":"演示命令12readme当前使用 pid=14750 jinfo:123查看Java进程的栈空间大小:jinfo - ThreadStackSize 14750查看是否使用了压缩指针:jinfo -flag UseCompressedOops 14750查看系统属性:jinfo -sysprops 14750 jstack: 1查看一个指定的Java进程中的线程的状态:jstack 14750 jstat: 1查看gc的信息:jstat -gcutil 14750 jmap&amp;mat 123空间中各个年龄段的空间的使用情况:jmap -heap 14750dump当前java运行状态jmap -dump:live,format=b,file=/fileName 14750 本文参考链接: http://www.javaranger.com/archives/1063","categories":[{"name":"jvm","slug":"jvm","permalink":"https://mykite.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://mykite.github.io/tags/jvm/"},{"name":"命令","slug":"命令","permalink":"https://mykite.github.io/tags/命令/"}]},{"title":"jvm内存模型","slug":"jvm内存模型","date":"2017-04-17T06:58:58.311Z","updated":"2017-04-17T06:59:53.474Z","comments":true,"path":"2017/04/17/jvm内存模型/","link":"","permalink":"https://mykite.github.io/2017/04/17/jvm内存模型/","excerpt":"","text":"内存模型方法区（methodArea）,java堆（heap）,java栈（stack）,本地方法栈（native Method Stack） 对象分为：年轻代(Young)、年老代(Tenured)、持久代(Perm)年轻代(Young)： 年轻代分三个区。一个Eden区，两个Survivor区。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制年老区(Tenured。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来 对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor去过来的对象。而且，Survivor区总有一个是空的。 Tenured（年老代）年老代存放从年轻代存活的对象。一般来说年老代存放的都是生命期较长的对象。Perm（持久代） 用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置。 持久代(Perm)持久代是指MethodArea，不属于Heap。 本文参考链接：http://www.javaranger.com/archives/472","categories":[{"name":"jvm","slug":"jvm","permalink":"https://mykite.github.io/categories/jvm/"}],"tags":[{"name":"jvm","slug":"jvm","permalink":"https://mykite.github.io/tags/jvm/"},{"name":"内存模型","slug":"内存模型","permalink":"https://mykite.github.io/tags/内存模型/"}]},{"title":"ArrayBlockingQueue源码解析","slug":"ArrayBlockingQueue源码解析","date":"2017-04-17T06:48:26.506Z","updated":"2017-04-17T06:56:57.319Z","comments":true,"path":"2017/04/17/ArrayBlockingQueue源码解析/","link":"","permalink":"https://mykite.github.io/2017/04/17/ArrayBlockingQueue源码解析/","excerpt":"","text":"什么是ArrayBlockingQueue ArrayBlockingQueue底层是由数组实现的定长阻塞队列(阻塞表示如果没有原始那么获取元素会阻塞当前线程) ArrayBlockingQueue用来干嘛 ArrayBlockingQueue一般用于生产者消费者模型业务(排队机制，先进先出) 源码解析数据的存储 12345678910111213141516public class ArrayBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; private static final long serialVersionUID = -817911632652898426L; /** The queued items 存储元素容器*/ final Object[] items; /** items index for next take, poll, peek or remove 使用过的元素 */ int takeIndex; /** items index for next put, offer, or add 添加过的元素 */ int putIndex; /** Number of elements in the queue 当前元素数量 */ int count; 数据的操作add1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public boolean add(E e) &#123; return super.add(e);&#125;super.addpublic boolean add(E e) &#123; if (offer(e)) return true; else throw new IllegalStateException(&quot;Queue full&quot;);&#125;public boolean offer(E e) &#123; checkNotNull(e);//ArrayBlockingQueue不能存储null对象 final ReentrantLock lock = this.lock;//插入操作线程安全 lock.lock(); try &#123; if (count == items.length)//如果当前count==items.length表示队列已经忙了，不能插入 return false; else &#123; insert(e);//插入元素 return true; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125;private void insert(E x) &#123; items[putIndex] = x;//第一次put为0 putIndex = inc(putIndex);//递增 ++count;//数量递增 notEmpty.signal();//通知获取原始方法可以进行获取&#125;final int inc(int i) &#123;//如果当前putIndex==items.length那么putIndex重新从零开始 return (++i == items.length) ? 0 : i;&#125;//同样为添加元素，lock.lockInterruptibly如果检测到有Thread.interrupted();会直接抛出异常public void put(E e) throws InterruptedException &#123; checkNotNull(e); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; while (count == items.length) notFull.await(); insert(e); &#125; finally &#123; lock.unlock(); &#125;&#125; remove123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public boolean remove(Object o) &#123; if (o == null) return false; final Object[] items = this.items; final ReentrantLock lock = this.lock; lock.lock(); try &#123; for (int i = takeIndex, k = count; k &gt; 0; i = inc(i), k--) &#123; if (o.equals(items[i])) &#123;//从头部开始遍历元素判断 removeAt(i); return true; &#125; &#125; return false; &#125; finally &#123; lock.unlock(); &#125;&#125;//queue size = 10 putSize = 5 tackSize = 0//queue 1,2,3,4,5removeAt 3step1: removeAt != takeIndexi = nexti = 4void removeAt(int i) &#123; final Object[] items = this.items; // if removing front item, just advance if (i == takeIndex) &#123; items[takeIndex] = null;//引用设置为空 takeIndex = inc(takeIndex);//takeIndex++ &#125; else &#123; // slide over all others up through putIndex. for (;;) &#123; int nexti = inc(i);//&gt;队列的头部 递增(putIndex一个循环的0-n) if (nexti != putIndex) &#123;//递增后部位putIndex全部向前移动位置 items[i] = items[nexti]; i = nexti; &#125; else &#123; items[i] = null;//元素设置为空 putIndex = i; break; &#125; &#125; &#125; --count;//元素递减 notFull.signal();//通知notFull.awit()&#125; update12```get public E poll() {//获取队列头部元素，获取后设置为空 final ReentrantLock lock = this.lock; lock.lock(); try { return (count == 0) ? null : extract();//如果当前队列为空直接返回null,不为空调用extract() } finally { lock.unlock(); }}//获取队列头部元素，获取后设置为空//take获取原始如果队列为空会进入阻塞状态知道等到有添加元素才会去返回public E take() throws InterruptedException { final ReentrantLock lock = this.lock; lock.lockInterruptibly();//lock.lockInterruptibly如果检测到有Thread.interrupted();会直接抛出异常 try { while (count == 0) notEmpty.await();//如果没有元素进入等待状态，等待被唤醒 return extract(); } finally { lock.unlock(); }}//peek查看队列头部元素public E peek() { final ReentrantLock lock = this.lock; lock.lock(); try { return (count == 0) ? null : itemAt(takeIndex);//如果元素为空直接返回null,不为空条用itemAt(takeIndex) } finally { lock.unlock(); }}private E extract() { final Object[] items = this.items; E x = this.cast(items[takeIndex]);//泛型转换并且获得当前元素 items[takeIndex] = null;//当前元素设置为空 takeIndex = inc(takeIndex);//获取原始递增 –count;//队列元素递减 notFull.signal();//通知notFull.await()可以进行插入元素 return x;//返回当前获取原始}//获取元素final E itemAt(int i) { return this.cast(items[i]);}``` 什么时候扩容 定长队列，不能进行扩容 是否线程安全 线程安全 使用注意事项 ArrayBlockingQueue为定长队列 ArrayBlockingQueue的添加和获取方法都有提供阻塞和非阻塞的根据需要使用 引用 jdk源码","categories":[{"name":"Collection","slug":"Collection","permalink":"https://mykite.github.io/categories/Collection/"},{"name":"queue","slug":"Collection/queue","permalink":"https://mykite.github.io/categories/Collection/queue/"}],"tags":[{"name":"queue","slug":"queue","permalink":"https://mykite.github.io/tags/queue/"},{"name":"源码分析","slug":"源码分析","permalink":"https://mykite.github.io/tags/源码分析/"},{"name":"blockngQueue","slug":"blockngQueue","permalink":"https://mykite.github.io/tags/blockngQueue/"},{"name":"java","slug":"java","permalink":"https://mykite.github.io/tags/java/"}]},{"title":"LinkedBlockingQueue源码解析","slug":"LinkedBlockingQueue源码解析","date":"2017-04-17T06:47:16.504Z","updated":"2017-04-17T06:57:03.583Z","comments":true,"path":"2017/04/17/LinkedBlockingQueue源码解析/","link":"","permalink":"https://mykite.github.io/2017/04/17/LinkedBlockingQueue源码解析/","excerpt":"","text":"什么是LinkedBlockingQueue LinkedBlockingQueue底层是由节点链表实现的定长阻塞队列(阻塞表示如果没有原始那么获取元素会阻塞当前线程) LinkedBlockingQueue用来干嘛 LinkedBlockingQueue一般用于生产者消费者模型业务(排队机制，先进先出) 源码解析数据的存储123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class LinkedBlockingQueue&lt;E&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt;, java.io.Serializable &#123; private static final long serialVersionUID = -6903933977591709194L; /** * Linked list node class */ static class Node&lt;E&gt; &#123;//存储数据的节点 E item; Node&lt;E&gt; next; Node(E x) &#123; item = x; &#125; &#125; /** The capacity bound, or Integer.MAX_VALUE if none */ private final int capacity;//链表的最大长度，如果不设置值默认为Integer.MAX_VALUE /** Current number of elements */ private final AtomicInteger count = new AtomicInteger(0);//统计数量线程安全 /** * Head of linked list. * Invariant: head.item == null */ private transient Node&lt;E&gt; head;//头节点 /** * Tail of linked list. * Invariant: last.next == null */ private transient Node&lt;E&gt; last;//尾节点 /** Lock held by take, poll, etc */ private final ReentrantLock takeLock = new ReentrantLock();//tackLock /** Wait queue for waiting takes */ private final Condition notEmpty = takeLock.newCondition();//tackLock条件不为空 /** Lock held by put, offer, etc */ private final ReentrantLock putLock = new ReentrantLock();//putLock /** Wait queue for waiting puts */ private final Condition notFull = putLock.newCondition();//putLock条件没满 public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE); &#125; public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);//默认last=head=空节点 &#125; 数据的操作add123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException();//不能存储空元素 int c = -1; Node&lt;E&gt; node = new Node(e);//创建节点 final ReentrantLock putLock = this.putLock;//获得putLock final AtomicInteger count = this.count;//获取当前数量 putLock.lockInterruptibly();//获取锁，如果有调用Thread.Interrupted()直接抛出异常 try &#123; while (count.get() == capacity) &#123;//如果当前队列以满，进入等待状态 notFull.await(); &#125; enqueue(node); c = count.getAndIncrement(); if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) signalNotEmpty();&#125;public boolean offer(E e, long timeout, TimeUnit unit) offer(e)类似 throws InterruptedException &#123; if (e == null) throw new NullPointerException();//不能存储空元素 long nanos = unit.toNanos(timeout);//装换为纳秒 int c = -1; final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly(); try &#123; while (count.get() == capacity) &#123; if (nanos &lt;= 0) return false; nanos = notFull.awaitNanos(nanos);//等待一段时间 &#125; enqueue(new Node&lt;E&gt;(e)); c = count.getAndIncrement();//递增 if (c + 1 &lt; capacity)//如果未满唤醒notFull.awit notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125; if (c == 0) signalNotEmpty();//唤醒notEmpty.await() return true;&#125;private void enqueue(Node&lt;E&gt; node) &#123; // assert putLock.isHeldByCurrentThread(); // assert last.next == null; //拆分为两步 last.next = node，last = node //每次head.next=当前的last然后last.next指向node last = last.next = node; &#125; remove12345678910111213141516171819202122232425262728public boolean remove(Object o) &#123; if (o == null) return false; fullyLock();//删除数据时全部lock try &#123; for (Node&lt;E&gt; trail = head, p = trail.next; p != null; trail = p, p = p.next) &#123; if (o.equals(p.item)) &#123; unlink(p, trail); return true; &#125; &#125; return false; &#125; finally &#123; fullyUnlock(); &#125;&#125;void unlink(Node&lt;E&gt; p, Node&lt;E&gt; trail) &#123; // assert isFullyLocked(); // p.next is not changed, to allow iterators that are // traversing p to maintain their weak-consistency guarantee. p.item = null; trail.next = p.next;//前后元素执行，大年元素设置为空 if (last == p) last = trail; if (count.getAndDecrement() == capacity)//count获取数量同时递减(获取数量为递减钱数量) notFull.signal();//唤醒 notFull.await()&#125; get123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475//获取元素，消费，可能被中断public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly();//如果有调用Thread.Interrupted()抛出异常 try &#123; while (count.get() == 0) &#123; notEmpty.await();//元素为空进入等待状态 &#125; x = dequeue();// c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125;//获取元素，消费public E poll() &#123; final AtomicInteger count = this.count; if (count.get() == 0) return null; E x = null; int c = -1; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; if (count.get() &gt; 0) &#123; x = dequeue(); c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; &#125; finally &#123; takeLock.unlock(); &#125; if (c == capacity) signalNotFull(); return x;&#125;//查看元素public E peek() &#123; if (count.get() == 0) return null; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; Node&lt;E&gt; first = head.next; if (first == null) return null; else return first.item; &#125; finally &#123; takeLock.unlock(); &#125;&#125;[null,aaa,bbb] queue[null,bbb] delete after queue去掉头部null元素获取aaa元素修改aaa元素的item=nullprivate E dequeue() &#123; // assert takeLock.isHeldByCurrentThread(); // assert head.item == null; Node&lt;E&gt; h = head; Node&lt;E&gt; first = h.next;//first第一个有值的节点 h.next = h; // help GC head = first; E x = first.item;//获取元素 first.item = null;//设置为空 return x;&#125; 什么时候扩容 定长链表不支持扩容 是否线程安全 线程安全 使用注意事项 默认创建方式链表醉大长度为Ineger.MAX_SIZE 引用 jdk源码","categories":[{"name":"Collection","slug":"Collection","permalink":"https://mykite.github.io/categories/Collection/"},{"name":"queue","slug":"Collection/queue","permalink":"https://mykite.github.io/categories/Collection/queue/"}],"tags":[{"name":"queue","slug":"queue","permalink":"https://mykite.github.io/tags/queue/"},{"name":"BlocingQueue","slug":"BlocingQueue","permalink":"https://mykite.github.io/tags/BlocingQueue/"},{"name":"LinkedBlockingQueue","slug":"LinkedBlockingQueue","permalink":"https://mykite.github.io/tags/LinkedBlockingQueue/"},{"name":"源码分析","slug":"源码分析","permalink":"https://mykite.github.io/tags/源码分析/"}]},{"title":"List总结","slug":"List总结","date":"2017-04-17T06:16:01.950Z","updated":"2017-04-17T06:57:07.000Z","comments":true,"path":"2017/04/17/List总结/","link":"","permalink":"https://mykite.github.io/2017/04/17/List总结/","excerpt":"","text":"对比1为体现差距使用 快，中等，一般 对比 增加 删除 修改 查询 ArrayList 快 一般 快 快 LinkedList 快 快 快 一般 copyOnWriteList 一般 一般 一般 快 ArrayList删除元素设计到元素的移动 LinkedList只能遍历(first last除外) copyOnWriteList增加删除都设计到数组的拷贝 闲话1copyOnWriteList没有实际使用过，感觉应用场景不多","categories":[{"name":"Collection","slug":"Collection","permalink":"https://mykite.github.io/categories/Collection/"},{"name":"List","slug":"Collection/List","permalink":"https://mykite.github.io/categories/Collection/List/"}],"tags":[]},{"title":"LinkedList源码解析","slug":"LinkedList源码解析","date":"2017-04-17T06:15:30.565Z","updated":"2017-04-17T06:57:05.000Z","comments":true,"path":"2017/04/17/LinkedList源码解析/","link":"","permalink":"https://mykite.github.io/2017/04/17/LinkedList源码解析/","excerpt":"","text":"什么是LinkedList ArrayList底层是由链表组成的一种数据结构，可以进行动态的增删改查 LinkedList用来干嘛 LinkedList一般用于对数据的存储 源码解析 数据的存储 数据的操作 什么时候扩容 是否线程安全 带上问题去找答案 数据的存储1234567891011121314151617public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable&#123; transient int size = 0; /** * Pointer to first node. * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */ transient Node&lt;E&gt; first; /** * Pointer to last node. * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */ transient Node&lt;E&gt; last; 数据的操作add (addFirst addLast类似)12345678910111213141516171819202122232425public boolean add(E e) &#123; linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last;//获取last final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null);//创建Node last = newNode;//last为新的节点 //如果当前l为空，表示是第一次添加，那么first也会=新的节点 //如果第一次添加就是first=last=newNode if (l == null) first = newNode; //l不为空也就是说不是第一次添加 //当前的last=newNode,而现在由于创建Node的时候已经吧newNode.prev=last也就是说现在是维护双向的关系 else l.next = newNode; size++; modCount++;&#125;//创建Node参数prev上一个，element当前元素，next下一个。添加的时候给定prev为last,element为当前，next为空Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev;&#125; 删除对象 linkedList可以存储null1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public boolean remove(Object o) &#123; //删除元素为空从first开始遍历判断为空 if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false;&#125;/*解除关系1.将当前Node的prev next item都设置为空2.将prev节点的next直接指向next(如果prev为空将first指向next)3.强next节点的prev直接指向prev(如果next为空将last指向prev)*/E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item;//当前元素 final Node&lt;E&gt; next = x.next;//当前元素的下一个节点 final Node&lt;E&gt; prev = x.prev;//当前元素的上一个节点 if (prev == null) &#123;//如果上一个节点为空，那么first将直接指向next first = next; &#125; else &#123; prev.next = next;//当前元素不为空将prev的next直接指向当前元素的下一个节点() x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 获取1因为是链表结构，只支持getFirst,getLast 什么时候扩容 链表没有终点不需要扩容 是否线程安全 线程不安全 使用注意事项 LinkedList不支持直接定位到元素 引用 jdk源码","categories":[{"name":"Collection","slug":"Collection","permalink":"https://mykite.github.io/categories/Collection/"},{"name":"List","slug":"Collection/List","permalink":"https://mykite.github.io/categories/Collection/List/"}],"tags":[]},{"title":"CopyOnWriteArrayList源码解析","slug":"CopyOnWriteArrayList源码解析","date":"2017-04-17T06:14:59.397Z","updated":"2017-04-17T06:57:26.000Z","comments":true,"path":"2017/04/17/CopyOnWriteArrayList源码解析/","link":"","permalink":"https://mykite.github.io/2017/04/17/CopyOnWriteArrayList源码解析/","excerpt":"","text":"什么是CopyOnWriteArrayList CopyOnWriteArrayList底层是由数组组成的一种数据结构，可以进行动态的增删改查 CopyOnWriteArrayList用来干嘛 CopyOnWriteArrayList一般用于对数据的存储(最好针对少量数据，添加会涉及到整个数组的复制) 源码解析 数据的存储 数据的操作 什么时候扩容 是否线程安全 带上问题去找答案 数据的存储 123456789public class CopyOnWriteArrayList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = 8673264195747942595L; /** 用于实现add的同步操作 */ transient final ReentrantLock lock = new ReentrantLock(); /** volatile针对读取时获取最新值，同时作为容器 */ private volatile transient Object[] array; 数据的操作123456789101112131415161718192021222324252627282930313233343536373839404142添加public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123;//同步操作 Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1);//添加操作设计到整个数组的复制，影响性能 newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125;删除public E remove(int index) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123;//同步代码 Object[] elements = getArray(); int len = elements.length; E oldValue = get(elements, index); int numMoved = len - index - 1; if (numMoved == 0) setArray(Arrays.copyOf(elements, len - 1));//数组复制 else &#123; Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); &#125; return oldValue; &#125; finally &#123; lock.unlock(); &#125;&#125;获取public E get(int index) &#123;//基于volatile获取最新值 return get(getArray(), index);&#125; 什么时候扩容 每次添加删除，针对array做copy操作 是否线程安全基于Lock实现并发写入的安全，针对并发修改的读取，修改基于copy后的新数组，读取如果未set获取到的还是原数组。如果set后读取到的就是最新的值 使用注意事项 避免CopyOnWriteArrayList过长，copy影响性能 引用 jdk源码","categories":[{"name":"Collection","slug":"Collection","permalink":"https://mykite.github.io/categories/Collection/"},{"name":"List","slug":"Collection/List","permalink":"https://mykite.github.io/categories/Collection/List/"}],"tags":[{"name":"List","slug":"List","permalink":"https://mykite.github.io/tags/List/"},{"name":"集合框架","slug":"集合框架","permalink":"https://mykite.github.io/tags/集合框架/"},{"name":"CopyOnWriteArrayList","slug":"CopyOnWriteArrayList","permalink":"https://mykite.github.io/tags/CopyOnWriteArrayList/"}]},{"title":"ArrayList源码解析","slug":"ArrayList源码解析","date":"2017-04-17T06:14:18.183Z","updated":"2017-04-17T06:55:52.327Z","comments":true,"path":"2017/04/17/ArrayList源码解析/","link":"","permalink":"https://mykite.github.io/2017/04/17/ArrayList源码解析/","excerpt":"","text":"12345678910111213141516171819闲来无事,有空会继续写下ListLinkedListCopyOnWriteArrayListSetHashSetLinkedHashSetMapHashMapIdentityHashMapLinkedHashMapTreeMapConcurrentHashMapQueueArrayBlockingQueueLinkedBlockingQueueConcurrentLinkedQueue为节省空间对于非重点代码不做展示 什么是ArrayList ArrayList底层是由数组组成的一种数据结构，可以进行动态的增删改查 ArrayList用来干嘛 ArrayList一般用于对数据的存储 源码解析针对重要点 数据的存储 数据的操作 什么时候扩容 是否线程安全带上问题去找答案数据的存储123456789从我们使用ArrayList开始new ArrayList&lt;&gt;();public ArrayList() &#123; super(); this.elementData = EMPTY_ELEMENTDATA;&#125;private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;private transient Object[] elementData;elementData为储存数据所用容器，通过默认构造方法创建的ArrayList容器为空 数据的操作添加123456789101112131415161718public boolean add(E e) &#123; ensureCapacityInternal(size + 1); elementData[size++] = e;//size表示当前使用下表，直接将元素放入到elementData的指定位置 return true;&#125;指定位置的添加，设计到元素的后移 public void add(int index, E element) &#123; rangeCheckForAdd(index);//检查是否超出 ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index);//元素后裔 elementData[index] = element; size++;&#125;private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));//很常见的异常&#125; 删除123456789101112public E remove(int index) &#123; rangeCheck(index); checkForComodification(); E result = parent.remove(parentOffset + index); this.modCount = parent.modCount; this.size--; return result;&#125;final void checkForComodification() &#123; if (modCount != expectedModCount)//针对在对于list进行遍历时进行其他操作，modCount会改变，而expectedModCount值在listInterator时给定的会抛出异常 throw new ConcurrentModificationException();&#125; 查询1234 public E get(int index) &#123; rangeCheck(index);//这就没啥好多的了 return elementData(index);&#125; 什么时候扩容123456789101112131415161718192021222324252627282930313233343536 public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == EMPTY_ELEMENTDATA) &#123; //如果容器为空，初始化容器，两个值中取最大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++;//操作次数+1 // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //扩容，当前元素数量已经等于容器了需要进行扩容 grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length;//当前容器大小 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);// oldCapacity + oldCapacity*0.5 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity);//计算是否超出 // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);//扩容&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow //如果为负数抛出内存溢出错误 throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? //还没有为负数，那么元素最大大小改为int的最大值，注意MAX_ARRAY_SIZE为最大值-8 Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125;private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; 是否线程安全 ArrayList不是线程安全的 12345678 以添加为例 elementData[size++] = e; 一个 ArrayList ，在添加一个元素的时候，它可能会有两步来完成： 1. 在 Items[Size] 的位置存放此元素； 2. 增大 Size 的值。 在单线程运行的情况下，如果 Size = 0，添加一个元素后，此元素在位置 0，而且 Size=1； 而如果是在多线程情况下，比如有两个线程，线程 A 先将元素存放在位置 0。但是此时 CPU 调度线程A暂停，线程 B 得到运行的机会。线程B也向此 ArrayList 添加元素，因为此时 Size 仍然等于 0 （注意哦，我们假设的是添加一个元素是要两个步骤哦，而线程A仅仅完成了步骤1），所以线程B也将元素存放在位置0。然后线程A和线程B都继续运行，都增加 Size 的值。 那好，现在我们来看看 ArrayList 的情况，元素实际上只有一个，存放在位置 0，而 Size 却等于 2。这就是“线程不安全”了。 使用ArrayList的注意事项 arrayList不是线程安全的，不要有多个线程同时操作一个arrayList 不要在循环中对arrayList做其他操作，会引发异常 针对已经确定大小的List请直接传入参数，避免多次扩容 参考 为什么说ArrayList是线程不安全的？ jdk源码","categories":[{"name":"Collection","slug":"Collection","permalink":"https://mykite.github.io/categories/Collection/"},{"name":"List","slug":"Collection/List","permalink":"https://mykite.github.io/categories/Collection/List/"}],"tags":[]}]}